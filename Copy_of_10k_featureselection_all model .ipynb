{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 10k_featureselection_cnnlstm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TE2BHZAjEPfc",
        "3tpLiMqNK9qj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pepperamy/tenK_phase2/blob/main/Copy_of_10k_featureselection_all%20model%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gFdXv30e4Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30cfd95-f0fe-4dad-b515-44a1f67b7a85"
      },
      "source": [
        "import pandas as pd\r\n",
        "import pickle\r\n",
        "import re\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "from scipy.optimize import linear_sum_assignment\r\n",
        "import time\r\n",
        "\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "\r\n",
        "import statsmodels.api as sm\r\n",
        "import statsmodels.formula.api as smf\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "from scipy.stats import mstats\r\n",
        "import math\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPbGENVjzMx6"
      },
      "source": [
        "from sklearn.model_selection import KFold,StratifiedKFold"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovlMafoEj7fh"
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edodAjPPLihc"
      },
      "source": [
        "from keras import backend as K"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEyVpJiqfWh7"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, precision_recall_curve, classification_report,accuracy_score, auc, roc_curve, roc_auc_score, average_precision_score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQoE_A26fcCN"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from keras.layers import Embedding, Dense, Conv2D, MaxPooling2D, Reshape, Conv1D, MaxPooling1D,\\\r\n",
        "Dropout, Activation, Input, Flatten, Concatenate, BatchNormalization, Lambda, LSTM, GRU, Bidirectional,\\\r\n",
        "ZeroPadding2D\r\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\r\n",
        "from keras import regularizers\r\n",
        "from keras.models import Model\r\n",
        "from keras import optimizers\r\n",
        "from keras import metrics\r\n",
        "from keras import models\r\n",
        "from keras import layers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOhMLRyIB5rG"
      },
      "source": [
        "from sklearn.utils import class_weight"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU6Xry_KF88_"
      },
      "source": [
        "from sklearn import utils"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LTmS2gbJsCI"
      },
      "source": [
        "import random"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QGsL2oSKI0I"
      },
      "source": [
        "from sklearn.svm import LinearSVC,SVC\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\r\n",
        "#from sklearn import metrics\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP4mF35eC3Gv"
      },
      "source": [
        "# data prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGDo0BX3fdqF"
      },
      "source": [
        "df = pd.read_csv('data_performance_words_win1_comb_20210301.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXrMxcpfdcof",
        "outputId": "405295c9-ec7a-437a-c32c-bc901edbdf91"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53635, 212)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTHf-QumI8Cd",
        "outputId": "0a2324c0-55e2-482a-b98c-c345a5c9828f"
      },
      "source": [
        "sum(df.label)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "487.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "7uYp5m-amwON",
        "outputId": "a6bd61a5-6290-4401-bcac-b73d0bac6f16"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cik</th>\n",
              "      <th>y1</th>\n",
              "      <th>y2</th>\n",
              "      <th>s1</th>\n",
              "      <th>window</th>\n",
              "      <th>label</th>\n",
              "      <th>sic_class_x</th>\n",
              "      <th>rsst_acc</th>\n",
              "      <th>ch_rec</th>\n",
              "      <th>ch_inv</th>\n",
              "      <th>soft_asset</th>\n",
              "      <th>ch_cs</th>\n",
              "      <th>ch_roa</th>\n",
              "      <th>issue</th>\n",
              "      <th>aqi</th>\n",
              "      <th>asset_turnover</th>\n",
              "      <th>cfed</th>\n",
              "      <th>depi</th>\n",
              "      <th>gmi</th>\n",
              "      <th>ig</th>\n",
              "      <th>opm</th>\n",
              "      <th>rg</th>\n",
              "      <th>sg</th>\n",
              "      <th>sgee</th>\n",
              "      <th>pastavg5</th>\n",
              "      <th>pastavg3</th>\n",
              "      <th>pastavg1</th>\n",
              "      <th>cr5</th>\n",
              "      <th>cr3</th>\n",
              "      <th>cr1</th>\n",
              "      <th>WeakModal_3_avg</th>\n",
              "      <th>WeakModal_3_dis</th>\n",
              "      <th>WeakModal_3_n</th>\n",
              "      <th>WeakModal_3_new</th>\n",
              "      <th>WeakModal_3_p</th>\n",
              "      <th>WeakModal_3_u</th>\n",
              "      <th>Litigious_3_avg</th>\n",
              "      <th>Litigious_3_dis</th>\n",
              "      <th>Litigious_3_n</th>\n",
              "      <th>Litigious_3_new</th>\n",
              "      <th>...</th>\n",
              "      <th>Achieve_3_p</th>\n",
              "      <th>Achieve_3_u</th>\n",
              "      <th>Power_3_avg</th>\n",
              "      <th>Power_3_dis</th>\n",
              "      <th>Power_3_n</th>\n",
              "      <th>Power_3_new</th>\n",
              "      <th>Power_3_p</th>\n",
              "      <th>Power_3_u</th>\n",
              "      <th>Reward_3_avg</th>\n",
              "      <th>Reward_3_dis</th>\n",
              "      <th>Reward_3_n</th>\n",
              "      <th>Reward_3_new</th>\n",
              "      <th>Reward_3_p</th>\n",
              "      <th>Reward_3_u</th>\n",
              "      <th>Risk_3_avg</th>\n",
              "      <th>Risk_3_dis</th>\n",
              "      <th>Risk_3_n</th>\n",
              "      <th>Risk_3_new</th>\n",
              "      <th>Risk_3_p</th>\n",
              "      <th>Risk_3_u</th>\n",
              "      <th>WeakModal_up</th>\n",
              "      <th>WeakModal_down</th>\n",
              "      <th>Litigious_up</th>\n",
              "      <th>Litigious_down</th>\n",
              "      <th>StrongModal_up</th>\n",
              "      <th>StrongModal_down</th>\n",
              "      <th>Negative_up</th>\n",
              "      <th>Negative_down</th>\n",
              "      <th>Positive_up</th>\n",
              "      <th>Positive_down</th>\n",
              "      <th>Uncertainty_up</th>\n",
              "      <th>Uncertainty_down</th>\n",
              "      <th>Compare_up</th>\n",
              "      <th>Compare_down</th>\n",
              "      <th>Achieve_up</th>\n",
              "      <th>Achieve_down</th>\n",
              "      <th>Discrep_up</th>\n",
              "      <th>Discrep_down</th>\n",
              "      <th>Reward_up</th>\n",
              "      <th>Reward_down</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20.0</td>\n",
              "      <td>1995.0</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>0.633505</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.063117</td>\n",
              "      <td>-0.072425</td>\n",
              "      <td>-0.067847</td>\n",
              "      <td>0.661974</td>\n",
              "      <td>-0.147079</td>\n",
              "      <td>0.198730</td>\n",
              "      <td>1</td>\n",
              "      <td>0.974056</td>\n",
              "      <td>1.624273</td>\n",
              "      <td>0.206066</td>\n",
              "      <td>1.226305</td>\n",
              "      <td>0.914198</td>\n",
              "      <td>0.715334</td>\n",
              "      <td>0.044798</td>\n",
              "      <td>0.783539</td>\n",
              "      <td>0.814093</td>\n",
              "      <td>1.013528</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000071</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>-0.002775</td>\n",
              "      <td>0.002801</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>-0.004527</td>\n",
              "      <td>0.016180</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.029638</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>-0.002433</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.019737</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.052570</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.193277</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.058512</td>\n",
              "      <td>0.353070</td>\n",
              "      <td>0.338308</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.292404</td>\n",
              "      <td>0.100189</td>\n",
              "      <td>0.145658</td>\n",
              "      <td>0.395804</td>\n",
              "      <td>0.355975</td>\n",
              "      <td>0.060777</td>\n",
              "      <td>0.303224</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.248916</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>0.193121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.0</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>1997.0</td>\n",
              "      <td>0.803827</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.066126</td>\n",
              "      <td>-0.018252</td>\n",
              "      <td>-0.059281</td>\n",
              "      <td>0.620435</td>\n",
              "      <td>-0.066028</td>\n",
              "      <td>0.034753</td>\n",
              "      <td>1</td>\n",
              "      <td>1.248039</td>\n",
              "      <td>1.606518</td>\n",
              "      <td>0.025826</td>\n",
              "      <td>1.088859</td>\n",
              "      <td>0.978102</td>\n",
              "      <td>0.755016</td>\n",
              "      <td>0.062466</td>\n",
              "      <td>0.938786</td>\n",
              "      <td>0.969746</td>\n",
              "      <td>1.012432</td>\n",
              "      <td>0.633505</td>\n",
              "      <td>0.633505</td>\n",
              "      <td>0.633505</td>\n",
              "      <td>1.268856</td>\n",
              "      <td>1.268856</td>\n",
              "      <td>1.268856</td>\n",
              "      <td>-0.000034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.003234</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>-0.003780</td>\n",
              "      <td>0.018289</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.007947</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>-0.001054</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.161905</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.161905</td>\n",
              "      <td>0.357243</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.219242</td>\n",
              "      <td>0.123810</td>\n",
              "      <td>0.134598</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.144428</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.075494</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.070482</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.207947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20.0</td>\n",
              "      <td>1997.0</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>0.462705</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.095900</td>\n",
              "      <td>0.066711</td>\n",
              "      <td>0.013223</td>\n",
              "      <td>0.656729</td>\n",
              "      <td>-0.030720</td>\n",
              "      <td>0.019574</td>\n",
              "      <td>1</td>\n",
              "      <td>0.840540</td>\n",
              "      <td>1.574474</td>\n",
              "      <td>0.020191</td>\n",
              "      <td>0.991780</td>\n",
              "      <td>0.991078</td>\n",
              "      <td>1.073227</td>\n",
              "      <td>0.073961</td>\n",
              "      <td>1.241132</td>\n",
              "      <td>1.022834</td>\n",
              "      <td>0.983269</td>\n",
              "      <td>0.718666</td>\n",
              "      <td>0.718666</td>\n",
              "      <td>0.803827</td>\n",
              "      <td>0.643838</td>\n",
              "      <td>0.643838</td>\n",
              "      <td>0.575627</td>\n",
              "      <td>0.002823</td>\n",
              "      <td>0.005714</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.007841</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>-0.001499</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.028035</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.021889</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>-0.003310</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>-0.000069</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.004688</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.141174</td>\n",
              "      <td>0.072381</td>\n",
              "      <td>0.137649</td>\n",
              "      <td>0.137333</td>\n",
              "      <td>0.074024</td>\n",
              "      <td>0.139048</td>\n",
              "      <td>0.149893</td>\n",
              "      <td>0.299598</td>\n",
              "      <td>0.070786</td>\n",
              "      <td>0.271178</td>\n",
              "      <td>0.154181</td>\n",
              "      <td>0.144762</td>\n",
              "      <td>0.484144</td>\n",
              "      <td>0.236060</td>\n",
              "      <td>0.074921</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.076624</td>\n",
              "      <td>0.202532</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>0.268170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.0</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>0.701689</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.054997</td>\n",
              "      <td>0.026323</td>\n",
              "      <td>-0.009875</td>\n",
              "      <td>0.676757</td>\n",
              "      <td>0.011434</td>\n",
              "      <td>0.002424</td>\n",
              "      <td>1</td>\n",
              "      <td>0.837833</td>\n",
              "      <td>1.604656</td>\n",
              "      <td>0.003715</td>\n",
              "      <td>1.025961</td>\n",
              "      <td>1.024634</td>\n",
              "      <td>0.948804</td>\n",
              "      <td>0.076906</td>\n",
              "      <td>1.077020</td>\n",
              "      <td>0.985921</td>\n",
              "      <td>0.983057</td>\n",
              "      <td>0.633346</td>\n",
              "      <td>0.633346</td>\n",
              "      <td>0.462705</td>\n",
              "      <td>1.107909</td>\n",
              "      <td>1.107909</td>\n",
              "      <td>1.516495</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.006755</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>-0.000112</td>\n",
              "      <td>0.005460</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.034894</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.019415</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.003575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008179</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>0.003620</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.052210</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.096370</td>\n",
              "      <td>0.048976</td>\n",
              "      <td>0.140434</td>\n",
              "      <td>0.093257</td>\n",
              "      <td>0.110468</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.050488</td>\n",
              "      <td>0.187374</td>\n",
              "      <td>0.063963</td>\n",
              "      <td>0.195518</td>\n",
              "      <td>0.159758</td>\n",
              "      <td>0.101197</td>\n",
              "      <td>0.055574</td>\n",
              "      <td>0.103194</td>\n",
              "      <td>0.144520</td>\n",
              "      <td>0.053633</td>\n",
              "      <td>0.003575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>0.823023</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.024892</td>\n",
              "      <td>-0.024929</td>\n",
              "      <td>0.041267</td>\n",
              "      <td>0.705573</td>\n",
              "      <td>-0.001713</td>\n",
              "      <td>-0.014429</td>\n",
              "      <td>1</td>\n",
              "      <td>1.093648</td>\n",
              "      <td>1.560280</td>\n",
              "      <td>-0.015406</td>\n",
              "      <td>0.942174</td>\n",
              "      <td>0.995699</td>\n",
              "      <td>1.221034</td>\n",
              "      <td>0.068754</td>\n",
              "      <td>0.933610</td>\n",
              "      <td>0.966150</td>\n",
              "      <td>1.000184</td>\n",
              "      <td>0.650432</td>\n",
              "      <td>0.656074</td>\n",
              "      <td>0.701689</td>\n",
              "      <td>1.265350</td>\n",
              "      <td>1.254468</td>\n",
              "      <td>1.172917</td>\n",
              "      <td>-0.000791</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>-0.000278</td>\n",
              "      <td>0.003717</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>-0.001826</td>\n",
              "      <td>0.011784</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.054348</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.007639</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>-0.000721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.178554</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.121364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.068988</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.184109</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.312405</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.129003</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.253588</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.007639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 212 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    cik      y1      y2  ...  Discrep_down  Reward_up  Reward_down\n",
              "0  20.0  1995.0  1996.0  ...      0.248916   0.190476     0.193121\n",
              "1  20.0  1996.0  1997.0  ...      0.070482   0.200000     0.207947\n",
              "2  20.0  1997.0  1998.0  ...      0.202532   0.002960     0.268170\n",
              "3  20.0  1998.0  1999.0  ...      0.144520   0.053633     0.003575\n",
              "4  20.0  1999.0  2000.0  ...      0.253588   0.176471     0.007639\n",
              "\n",
              "[5 rows x 212 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFPQxWfzmxA5"
      },
      "source": [
        "df_fl = df[(df.y2 <= 2012) & (df.y2 >= 1995 ) ]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHk9KoNgdKnU",
        "outputId": "60032cfd-becb-4d28-9e73-1ba217531da2"
      },
      "source": [
        "df_fl.label.value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    53148\n",
              "1.0      487\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htklHIW9JDXK",
        "outputId": "bdf66e90-5a02-49b1-a3b1-e8b7e4724fac"
      },
      "source": [
        "df_fl.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53635, 212)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czKM_c73sGzE"
      },
      "source": [
        "selected_new = ['WeakModal','Negative', 'Compare', 'Discrep','Positive',\\\r\n",
        "         'Achieve',  'Reward', 'StrongModal','Uncertainty', 'Litigious'][::-1]\r\n",
        "\r\n",
        "v_perf = ['aqi',\r\n",
        " 'asset_turnover',\r\n",
        " 'depi',\r\n",
        " 'gmi',\r\n",
        " #'ig',\r\n",
        " 'opm',\r\n",
        " 'rg',\r\n",
        " 'sg',\r\n",
        " 'sgee',\r\n",
        " 'ch_rec',\r\n",
        " 'ch_inv',\r\n",
        " 'soft_asset',\r\n",
        " 'ch_cs',\r\n",
        " 'ch_roa',\r\n",
        " 'issue']\r\n",
        "\r\n",
        "v_1 = ['s1']\r\n",
        "v_2 = ['pastavg3','cr3']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqB8Sa2NDeFF"
      },
      "source": [
        "# selected_comb = []\r\n",
        "# for s in selected_new:\r\n",
        "#   selected_comb.append(s+'_up')\r\n",
        "#   selected_comb.append(s+'_down')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u09YX6QTsRjF"
      },
      "source": [
        "selected_new_all = []\r\n",
        "temp = []\r\n",
        "for s in selected_new:\r\n",
        "    wrd = s.split('_')[0]\r\n",
        "    if wrd not in temp:\r\n",
        "        #print(s,'\\n',temp)\r\n",
        "        #selected_new_all.append(wrd+'_3_avg')\r\n",
        "        selected_new_all.append(wrd+'_3_p')\r\n",
        "        selected_new_all.append(wrd+'_3_n')\r\n",
        "        #selected_new_all.append(wrd+'_3_u')\r\n",
        "        selected_new_all.append(wrd+'_3_new')\r\n",
        "        selected_new_all.append(wrd+'_3_dis')\r\n",
        "        temp.append(wrd)\r\n",
        "    else: \r\n",
        "        pass"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08oZ_gShupro"
      },
      "source": [
        "selected_new_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLPx7-Q37_ft"
      },
      "source": [
        "selected_new_all_sorted = []\r\n",
        "selected_new_all_p = []\r\n",
        "selected_new_all_n = []\r\n",
        "selected_new_all_new = []\r\n",
        "selected_new_all_dis = []\r\n",
        "for s in selected_new_all:\r\n",
        "  if 'new' in s.split('_'):\r\n",
        "    selected_new_all_new.append(s)\r\n",
        "  elif 'p' in s.split('_'):\r\n",
        "    selected_new_all_p.append(s)\r\n",
        "  elif 'n' in s.split('_'):\r\n",
        "    selected_new_all_n.append(s)\r\n",
        "  elif 'dis' in s.split('_'):\r\n",
        "    selected_new_all_dis.append(s)\r\n",
        "\r\n",
        "for i, w in enumerate(selected_new_all_p):\r\n",
        "  selected_new_all_sorted.append(selected_new_all_p[i])\r\n",
        "  selected_new_all_sorted.append(selected_new_all_new[i])\r\n",
        "  selected_new_all_sorted.append(selected_new_all_n[i])\r\n",
        "  \r\n",
        "  selected_new_all_sorted.append(selected_new_all_dis[i])  \r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uuAH4EArW1P"
      },
      "source": [
        "selected_new_all_sorted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_jHGUWwCi3M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE2BHZAjEPfc"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWbZ3_n5Ci5x"
      },
      "source": [
        "def svm( data, vs, label = 'label'):\r\n",
        "    \r\n",
        "    columns_fl = vs[1] #+ [ 'sic_class_x']\r\n",
        "    \r\n",
        "    data_target = data.loc[:,columns_fl + [label]]\r\n",
        "    \r\n",
        "    tprs = []\r\n",
        "    auc_list = []\r\n",
        "    mean_fpr = np.linspace(0,1,100)\r\n",
        "    \r\n",
        "    class_report = []\r\n",
        "    sum_pred_list = []\r\n",
        "\r\n",
        "    kf = StratifiedKFold(n_splits = 4, shuffle = True)\r\n",
        "    c = 0\r\n",
        "    data_target = data[columns_fl + [label]]\r\n",
        "    data_target = data_target.dropna()\r\n",
        "    data_target = data_target.reset_index(drop = True)\r\n",
        "    print(data_target.label.value_counts())\r\n",
        "     \r\n",
        "    for train_index, test_index in kf.split(data_target[columns_fl],data_target[label]):\r\n",
        "        \r\n",
        "        c += 1\r\n",
        "\r\n",
        "        train_x = data_target.loc[train_index,columns_fl]\r\n",
        "        train_y = data_target.loc[train_index,label]\r\n",
        "        test_x = data_target.loc[test_index,columns_fl]\r\n",
        "        test_y = data_target.loc[test_index,label] \r\n",
        "\r\n",
        "        print('iterate_num: ', c, '\\n','sum of test_y:', sum(test_y))\r\n",
        "    \r\n",
        "        svm = LinearSVC(class_weight = \"balanced\")\r\n",
        "        svm.fit(train_x, train_y )\r\n",
        "        pickle.dump(svm, open('svm_'+str(vs[0])+ '_'+str(c), 'wb'))\r\n",
        "        \r\n",
        "\r\n",
        "        pred = svm.predict(test_x)\r\n",
        "        sum_pred_list.append(pred)\r\n",
        "        print('sum of pred: ', sum(pred), '\\n')\r\n",
        "        c_r = metrics.classification_report(test_y, pred, labels=[0,1], output_dict = True)\r\n",
        "        class_report.append(c_r)\r\n",
        "        #print(metrics.classification_report(test_y, pred, labels=[0,1]))\r\n",
        "        \r\n",
        "        decision_values = svm.decision_function(test_x)\r\n",
        "        auc_score = roc_auc_score(test_y, decision_values)\r\n",
        "        print(auc_score)\r\n",
        "        auc_list.append(auc_score)\r\n",
        "        \r\n",
        "        fpr, tpr, thresholds = roc_curve(test_y, decision_values)\r\n",
        "        tprs.append(np.interp(mean_fpr, fpr, tpr))\r\n",
        "        \r\n",
        "#         mean_tpr = np.mean(tprs, axis=0)\r\n",
        "#         mean_auc = auc(mean_fpr, mean_tpr)\r\n",
        "        \r\n",
        "        \r\n",
        "    mean_tpr = np.mean(tprs, axis=0)\r\n",
        "    print('avg_AUC : ', np.mean(auc_list))\r\n",
        "    print('avg_AUC_2 : ', auc(mean_fpr, mean_tpr))\r\n",
        "    frauds_prec = []\r\n",
        "    fradus_recall = []\r\n",
        "    for d in class_report:\r\n",
        "        frauds_prec.append(d['1']['precision'])\r\n",
        "        fradus_recall.append(d['1']['recall'])\r\n",
        "    \r\n",
        "#     coef_s = 0\r\n",
        "#     for i in coef:\r\n",
        "#         coef_s += abs(i)\r\n",
        "#     coef_avg = coef_s / len(coef)\r\n",
        "#     coef_top = pd.DataFrame(coef_avg, index = columns_fl,columns = ['importance'])\r\n",
        "#     coef_top = coef_top.sort_values('importance',ascending=False)\r\n",
        "#     print(coef_top.iloc[0:10,:])\r\n",
        "        \r\n",
        "    print('frauds_prec : ', np.mean(frauds_prec))\r\n",
        "    print('fradus_recall : ', np.mean(fradus_recall))\r\n",
        "    print('\\n')\r\n",
        "    return np.mean(auc_list),mean_tpr"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAx--SZdCi8A",
        "outputId": "6c7ba17f-23fa-4876-c3c1-186ad968afe5"
      },
      "source": [
        "vs_fl = {'perf':v_perf, 'perf+spc': v_perf +v_1+v_2, 'wrd+spc': selected_new_all +v_1 +v_2,'perf+spc+wrd': v_perf +v_1 +v_2 +selected_new_all}\r\n",
        "mean_tpr_dict_svm = dict()\r\n",
        "for fl in vs_fl.items():\r\n",
        "#    print(fl)\r\n",
        "    svm_model = svm( df_fl, fl,label = 'label')\r\n",
        "    print(fl)\r\n",
        "    print(svm_model[0])\r\n",
        "    mean_tpr_dict_svm[fl[0]] = svm_model[1]\r\n",
        "    print('============================')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0    53148\n",
            "1.0      487\n",
            "Name: label, dtype: int64\n",
            "iterate_num:  1 \n",
            " sum of test_y: 122.0\n",
            "sum of pred:  2885.0 \n",
            "\n",
            "0.6642768045186531\n",
            "iterate_num:  2 \n",
            " sum of test_y: 122.0\n",
            "sum of pred:  3303.0 \n",
            "\n",
            "0.6832457955329195\n",
            "iterate_num:  3 \n",
            " sum of test_y: 122.0\n",
            "sum of pred:  3269.0 \n",
            "\n",
            "0.6809145386776425\n",
            "iterate_num:  4 \n",
            " sum of test_y: 121.0\n",
            "sum of pred:  3063.0 \n",
            "\n",
            "0.6925454383735548\n",
            "avg_AUC :  0.6802456442756925\n",
            "avg_AUC_2 :  0.6805165528746344\n",
            "frauds_prec :  0.017430809883589825\n",
            "fradus_recall :  0.44775436932664947\n",
            "\n",
            "\n",
            "('perf', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue'])\n",
            "0.6802456442756925\n",
            "============================\n",
            "0.0    45204\n",
            "1.0      407\n",
            "Name: label, dtype: int64\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  1944.0 \n",
            "\n",
            "0.6847146964263097\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  1892.0 \n",
            "\n",
            "0.7080685207451708\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  1899.0 \n",
            "\n",
            "0.6666718718281046\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "sum of pred:  2022.0 \n",
            "\n",
            "0.6213180118117997\n",
            "avg_AUC :  0.6701932752028462\n",
            "avg_AUC_2 :  0.6703067365560086\n",
            "frauds_prec :  0.019123187139197122\n",
            "fradus_recall :  0.3635216462822753\n",
            "\n",
            "\n",
            "('perf+spc', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3'])\n",
            "0.6701932752028462\n",
            "============================\n",
            "0.0    45204\n",
            "1.0      407\n",
            "Name: label, dtype: int64\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4131.0 \n",
            "\n",
            "0.6409991480885779\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4003.0 \n",
            "\n",
            "0.588612668321908\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4043.0 \n",
            "\n",
            "0.6239600521210165\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "sum of pred:  4082.0 \n",
            "\n",
            "0.6374052589755922\n",
            "avg_AUC :  0.6227442818767737\n",
            "avg_AUC_2 :  0.622662511349174\n",
            "frauds_prec :  0.013149025733362907\n",
            "fradus_recall :  0.5258444962143273\n",
            "\n",
            "\n",
            "('wrd+spc', ['Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis', 's1', 'pastavg3', 'cr3'])\n",
            "0.6227442818767737\n",
            "============================\n",
            "0.0    45204\n",
            "1.0      407\n",
            "Name: label, dtype: int64\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  1995.0 \n",
            "\n",
            "0.704393676769885\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  2097.0 \n",
            "\n",
            "0.758595022824633\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  1977.0 \n",
            "\n",
            "0.6680833381047313\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "sum of pred:  2245.0 \n",
            "\n",
            "0.6603516205084803\n",
            "avg_AUC :  0.6978559145519325\n",
            "avg_AUC_2 :  0.6978963827755325\n",
            "frauds_prec :  0.020145985439373043\n",
            "fradus_recall :  0.4128324597165599\n",
            "\n",
            "\n",
            "('perf+spc+wrd', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3', 'Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis'])\n",
            "0.6978559145519325\n",
            "============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zESDM53TKweY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHiIenY9K8Lf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tpLiMqNK9qj"
      },
      "source": [
        "# LR\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q86DE7q6K8OH"
      },
      "source": [
        "def lg( data,vs, label = 'label'):\r\n",
        "    \r\n",
        "    \r\n",
        "    columns_fl = vs[1]# + [ 'sic_class_x']\r\n",
        "    data_target = data.loc[:,columns_fl + [label]]\r\n",
        "    \r\n",
        "        \r\n",
        "#     train_x, test_x, train_y, test_y = train_test_split(data_x, data_y,test_size=0.2, random_state=19, \r\n",
        "#                                                                         stratify = data_y) \r\n",
        "\r\n",
        "    tprs = []\r\n",
        "    auc_list = []\r\n",
        "    mean_fpr = np.linspace(0,1,100)\r\n",
        "    \r\n",
        "    class_report = []\r\n",
        "    sum_pred_list = []\r\n",
        "    kf = StratifiedKFold(n_splits = 4, shuffle = True)\r\n",
        "    c = 0\r\n",
        "    data_target = data[columns_fl + [label]]\r\n",
        "    data_target = data_target.dropna()\r\n",
        "    data_target = data_target.reset_index(drop = True)\r\n",
        "    for train_index, test_index in kf.split(data_target[columns_fl],data_target[label]):\r\n",
        "        \r\n",
        "        c += 1\r\n",
        "\r\n",
        "        train_x = data_target.loc[train_index,columns_fl]\r\n",
        "        train_y = data_target.loc[train_index,label]\r\n",
        "        test_x = data_target.loc[test_index,columns_fl]\r\n",
        "        test_y = data_target.loc[test_index,label] \r\n",
        "\r\n",
        "        print('iterate_num: ', c, '\\n','sum of test_y:', sum(test_y))\r\n",
        "    \r\n",
        "        lg = LogisticRegression(class_weight = 'balanced')\r\n",
        "        lg.fit(train_x, train_y )\r\n",
        "        #pickle.dump(lg, open('lg_'+str(vs[0])+ '_'+str(c), 'wb'))\r\n",
        "\r\n",
        "        pred = lg.predict(test_x)\r\n",
        "        sum_pred_list.append(pred)\r\n",
        "        print('sum of pred: ', sum(pred), '\\n')\r\n",
        "        c_r = metrics.classification_report(test_y, pred, labels=[0,1], output_dict = True)\r\n",
        "        class_report.append(c_r)\r\n",
        "        #print(metrics.classification_report(test_y, pred, labels=[0,1]))\r\n",
        "        \r\n",
        "        decision_values = lg.predict_proba(test_x)\r\n",
        "        auc_score = roc_auc_score(test_y, decision_values[:,1])\r\n",
        "        print(auc_score)\r\n",
        "        auc_list.append(auc_score)\r\n",
        "        fpr, tpr, thresholds = roc_curve(test_y, decision_values[:,1])\r\n",
        "        tprs.append(np.interp(mean_fpr, fpr, tpr))\r\n",
        "        \r\n",
        "        #print('AUC',auc_score)\r\n",
        "        \r\n",
        "    mean_tpr = np.mean(tprs, axis=0)\r\n",
        "    print('avg_AUC : ', np.mean(auc_list))\r\n",
        "    print('avg_AUC_2 : ', auc(mean_fpr, mean_tpr))\r\n",
        "    frauds_prec = []\r\n",
        "    fradus_recall = []\r\n",
        "    for d in class_report:\r\n",
        "        frauds_prec.append(d['1']['precision'])\r\n",
        "        fradus_recall.append(d['1']['recall'])\r\n",
        "        \r\n",
        "    print('frauds_prec : ', np.mean(frauds_prec))\r\n",
        "    print('fradus_recall : ', np.mean(fradus_recall))\r\n",
        "    print('\\n')\r\n",
        "    return np.mean(auc_list),mean_tpr"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgQ54FUiLAJt",
        "outputId": "ec339d5f-81eb-4151-d901-bdee00fc0bec"
      },
      "source": [
        "vs_fl = {'perf':v_perf, 'perf+spc': v_perf +v_1+v_2, 'wrd+spc': selected_new_all +v_1 +v_2,'perf+spc+wrd': v_perf+v_1+v_2+selected_new_all}\r\n",
        "mean_tpr_dict_lg = dict()\r\n",
        "for fl in vs_fl.items():\r\n",
        "    lg_model = lg( df_fl, fl,label = 'label')\r\n",
        "    print(fl)\r\n",
        "    print(lg_model[0])\r\n",
        "    mean_tpr_dict_lg[fl[0]] = lg_model[1]\r\n",
        "    print('============================')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterate_num:  1 \n",
            " sum of test_y: 122.0\n",
            "sum of pred:  5340.0 \n",
            "\n",
            "0.6703390593788826\n",
            "iterate_num:  2 \n",
            " sum of test_y: 122.0\n",
            "sum of pred:  5158.0 \n",
            "\n",
            "0.6768781762526418\n",
            "iterate_num:  3 \n",
            " sum of test_y: 122.0\n",
            "sum of pred:  5217.0 \n",
            "\n",
            "0.6674303861656963\n",
            "iterate_num:  4 \n",
            " sum of test_y: 121.0\n",
            "sum of pred:  5297.0 \n",
            "\n",
            "0.6779882405408381\n",
            "avg_AUC :  0.6731589655845147\n",
            "avg_AUC_2 :  0.6733243216612679\n",
            "frauds_prec :  0.014709944743580906\n",
            "fradus_recall :  0.6345007451564829\n",
            "\n",
            "\n",
            "('perf', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue'])\n",
            "0.6731589655845147\n",
            "============================\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4623.0 \n",
            "\n",
            "0.7197523731198523\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4405.0 \n",
            "\n",
            "0.6156925207035296\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4181.0 \n",
            "\n",
            "0.6563994857300499\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "sum of pred:  4509.0 \n",
            "\n",
            "0.7042152582659381\n",
            "avg_AUC :  0.6740149094548424\n",
            "avg_AUC_2 :  0.6741240300500638\n",
            "frauds_prec :  0.014593700058840902\n",
            "fradus_recall :  0.636526887982916\n",
            "\n",
            "\n",
            "('perf+spc', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3'])\n",
            "0.6740149094548424\n",
            "============================\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4323.0 \n",
            "\n",
            "0.6420575309143213\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4064.0 \n",
            "\n",
            "0.6224653032613807\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4236.0 \n",
            "\n",
            "0.6205307182602268\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "sum of pred:  4285.0 \n",
            "\n",
            "0.6186712645249128\n",
            "avg_AUC :  0.6259312042402103\n",
            "avg_AUC_2 :  0.6257348774093096\n",
            "frauds_prec :  0.0131310195650905\n",
            "fradus_recall :  0.545403805086391\n",
            "\n",
            "\n",
            "('wrd+spc', ['Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis', 's1', 'pastavg3', 'cr3'])\n",
            "0.6259312042402103\n",
            "============================\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  3858.0 \n",
            "\n",
            "0.6660264318097826\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  3979.0 \n",
            "\n",
            "0.6799840722059994\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "sum of pred:  4044.0 \n",
            "\n",
            "0.7472616513201157\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "sum of pred:  3906.0 \n",
            "\n",
            "0.7097426758869144\n",
            "avg_AUC :  0.700753707805703\n",
            "avg_AUC_2 :  0.7006533986732005\n",
            "frauds_prec :  0.016707706683578297\n",
            "fradus_recall :  0.648611920015531\n",
            "\n",
            "\n",
            "('perf+spc+wrd', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3', 'Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis'])\n",
            "0.700753707805703\n",
            "============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdVMNLDaLAMR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R7MWNdnMexr"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ9W1cJrNOuu"
      },
      "source": [
        "def model_M1(n1,n2):\r\n",
        "    model_m1 = Sequential(name = 'M1')\r\n",
        "    model_m1.add(BatchNormalization())\r\n",
        "    model_m1.add(layers.Dense(n1,name = 'layer_1',activation='relu'))\r\n",
        "    model_m1.add(layers.Dropout(0.3))\r\n",
        "    #model_m1.add(layers.Dense(64,name = 'layer_2'))\r\n",
        "    model_m1.add(layers.Dense(n2,name = 'layer_2',activation='relu'))\r\n",
        "    model_m1.add(layers.Dense(1,activation='sigmoid'))\r\n",
        "    #model_m1.summary()\r\n",
        "    return model_m1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mapP-xQTNTMv"
      },
      "source": [
        "def performance_measure(pred_yp, y):\r\n",
        "    '''\r\n",
        "    Given lists of predicted y probability and x, y, return a dataframe of AR, AUC, Brier, Decile Table\r\n",
        "    '''\r\n",
        "    \r\n",
        "    tenc_dat = pd.DataFrame({'y_true':y,'probability':pred_yp.flatten()})\r\n",
        "    tenc_dat.sort_values('probability',axis = 0,ascending=False, inplace = True)\r\n",
        "    tenc_dat.index = range(0,len(tenc_dat))\r\n",
        "    y = tenc_dat['y_true']\r\n",
        "    point = float(len(tenc_dat))/10\r\n",
        "    point = int(round(point))\r\n",
        "    tenc = []\r\n",
        "    for i in range(0,10):\r\n",
        "        tenc.append(y[(i*point):((i+1)*point)])\r\n",
        "    tenc[9]=tenc[9].append(y[10*point:])\r\n",
        "    total = sum(y)\r\n",
        "    num_of_bkr = []\r\n",
        "    for j in range(0,10):\r\n",
        "        num_of_bkr.append(sum(tenc[j]))\r\n",
        "    tencile_bkr = np.array(num_of_bkr)\r\n",
        "    rate = tencile_bkr.astype(float)/total\r\n",
        "\r\n",
        "    return rate"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGDx2QzbNZcD"
      },
      "source": [
        "class AUCEvaluation(Callback):\r\n",
        "    \"\"\" Show AUC after interval number of epoches \"\"\"\r\n",
        "    def __init__(self, validation_data=(), interval=1):\r\n",
        "        super(Callback, self).__init__()\r\n",
        "        self.interval = interval\r\n",
        "        self.X_val, self.y_val = validation_data\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if epoch % self.interval == 0:\r\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\r\n",
        "            score = roc_auc_score(self.y_val, y_pred)\r\n",
        "            logs['auc'] = score\r\n",
        "            tencile=performance_measure(y_pred, self.y_val)\r\n",
        "            logs['tencile'] = tencile\r\n",
        "            print(\" epoch:{:d} auc: {:.4f}\".format(epoch, score))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XNsDIM5NdG0"
      },
      "source": [
        "def fit_model( model,x_train, y_train, x_val, y_val, x_test, y_test, model_name, verbose = 0):\r\n",
        "    \r\n",
        "    history = None\r\n",
        "    result = None\r\n",
        "    \r\n",
        "    model_df = model\r\n",
        "    \r\n",
        "    #class weight for test \r\n",
        "    class_weights = class_weight.compute_class_weight('balanced',\r\n",
        "                              np.unique(y_train),\r\n",
        "                              y_train)\r\n",
        "    class_weights = {l:c for l,c in zip(np.unique(y_train), class_weights)}\r\n",
        "    # class weight for val\r\n",
        "    val_class_weights = class_weight.compute_class_weight('balanced',\r\n",
        "                                np.unique(y_val),\r\n",
        "                                y_val)\r\n",
        "    val_class_weights = {l:c for l,c in zip(np.unique(y_val), val_class_weights)}\r\n",
        "    val_sample_weights = []\r\n",
        "    for y in y_val:\r\n",
        "        if y == 1:\r\n",
        "          val_sample_weights.append(val_class_weights[1])\r\n",
        "        else: val_sample_weights.append(val_class_weights[0])\r\n",
        "    val_sample_weights = np.asarray(val_sample_weights)\r\n",
        "    \r\n",
        "    \r\n",
        "    auc_eval = AUCEvaluation(validation_data=(x_val, y_val), interval=1)\r\n",
        "    earlyStopping = EarlyStopping(monitor='val_my_auc',patience = 3, \r\n",
        "                      verbose =verbose, mode ='max')\r\n",
        "    checkpoint = ModelCheckpoint(model_name,monitor='val_my_auc',verbose=verbose,\r\n",
        "              save_best_only=True, save_weights_only=True, mode ='max')\r\n",
        "\r\n",
        "    opt = optimizers.RMSprop(lr=0.0005)\r\n",
        "    model_df.compile(optimizer= opt,\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy', metrics.AUC(name='my_auc')])\r\n",
        "    \r\n",
        "    history = model_df.fit(x_train,\r\n",
        "                y_train,\r\n",
        "                epochs=30,\r\n",
        "                batch_size=512,\r\n",
        "                callbacks=[auc_eval, earlyStopping,checkpoint], #checkpoint\r\n",
        "                class_weight = class_weights, \\\r\n",
        "                validation_data=(x_val,y_val,val_sample_weights)) \r\n",
        "    model_df.load_weights(model_name)\r\n",
        "    y_pred = model_df.predict(x_test)\r\n",
        "    y_pred_1=np.reshape(y_pred, -1)\r\n",
        "    auc_score = roc_auc_score(y_test, y_pred_1)\r\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_1)\r\n",
        "    print(\"AUC: \" ,round(auc_score,4))\r\n",
        "        \r\n",
        "    return history, auc_score, fpr, tpr, thresholds, [y_test, y_pred_1]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjXb2Su8OHwC"
      },
      "source": [
        "def cross_val(n1, n2, data,label,columns_fl,model_name):\r\n",
        "    tprs = []\r\n",
        "    auc_list = []\r\n",
        "    mean_fpr = np.linspace(0,1,100)\r\n",
        "    \r\n",
        "    kf = StratifiedKFold(n_splits = 4, shuffle = True)\r\n",
        "    c = 0\r\n",
        "    data_1 = data[columns_fl + [label]]\r\n",
        "    data_1 = data_1.dropna()\r\n",
        "    data_target = data_1.reset_index(drop = True)\r\n",
        "    \r\n",
        "    predicted_res =[]\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    for train_index, test_index in kf.split(data_target[columns_fl],data_target[label]):\r\n",
        "        \r\n",
        "        c += 1\r\n",
        "\r\n",
        "        train_data = data_target.loc[train_index,columns_fl]\r\n",
        "        train_label = data_target.loc[train_index,label]\r\n",
        "        test_x = data_target.loc[test_index,columns_fl].to_numpy()\r\n",
        "        test_y = data_target.loc[test_index,label] \r\n",
        "\r\n",
        "        print('iterate_num: ', c, '\\n','sum of test_y:', sum(test_y))\r\n",
        "        train_x, val_x, train_y, val_y = train_test_split(train_data.to_numpy(), train_label.to_numpy(),test_size=0.2, \\\r\n",
        "                                                            random_state=42, stratify = train_label)\r\n",
        "        print('train_x_shape',train_x.shape)\r\n",
        "        model = model_M1(n1,n2)\r\n",
        "        mod_res = fit_model(model, train_x, train_y, val_x, val_y, test_x, test_y, model_name+'_'+str(c)+'_NN_model')\r\n",
        "        auc_list.append(mod_res[1])\r\n",
        "        tprs.append(np.interp(mean_fpr, mod_res[2], mod_res[3]))\r\n",
        "#         print(type(mod_res[-1]))\r\n",
        "#         print(len(mod_res[-1]))\r\n",
        "#         print(mod_res[-1])\r\n",
        "        temp_pred_res = [mod_res[-1],test_index]\r\n",
        "        #print(temp_pred_res)\r\n",
        "        predicted_res.append(temp_pred_res)\r\n",
        "        \r\n",
        "    mean_tpr = np.mean(tprs, axis=0)\r\n",
        "    print('avg_AUC : ', np.mean(auc_list))\r\n",
        "    print('avg_AUC_2 : ', auc(mean_fpr, mean_tpr))\r\n",
        "    \r\n",
        "    return np.mean(auc_list),mean_tpr, predicted_res"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC6UwbsmOMOO",
        "outputId": "98e1e583-df8f-4168-d970-29c4e0660f42"
      },
      "source": [
        "label = 'label'\r\n",
        "vs_fl = {'perf':v_perf, 'perf+spc': v_perf +v_1+v_2, 'wrd+spc': selected_new_all +v_1 +v_2,'perf+spc+wrd': v_perf +v_1 +v_2 +selected_new_all}\r\n",
        "#\r\n",
        "mean_tpr_dict_nn = dict()\r\n",
        "pred_res_nn = dict()\r\n",
        "for fl in vs_fl.items():\r\n",
        "    print(fl[1])\r\n",
        "    n1 = len(fl[1])*2\r\n",
        "    n2 = len(fl[1])\r\n",
        "    #M1 = model_M1(n1,n2)\r\n",
        "    NN = cross_val(n1, n2, df_fl, label, fl[1], fl[0])\r\n",
        "    print(fl)\r\n",
        "    print(NN[0])\r\n",
        "    mean_tpr_dict_nn[fl[0]] = NN[1]\r\n",
        "    pred_res_nn[fl[0]] = NN[2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue']\n",
            "iterate_num:  1 \n",
            " sum of test_y: 122.0\n",
            "train_x_shape (32180, 14)\n",
            "Epoch 1/30\n",
            "63/63 [==============================] - 3s 24ms/step - loss: 0.7030 - accuracy: 0.5836 - my_auc: 0.5588 - val_loss: 0.6930 - val_accuracy: 0.4172 - val_my_auc: 0.5044\n",
            " epoch:0 auc: 0.5028\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6649 - accuracy: 0.5973 - my_auc: 0.6450 - val_loss: 0.6821 - val_accuracy: 0.4778 - val_my_auc: 0.5864\n",
            " epoch:1 auc: 0.5858\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.6026 - my_auc: 0.6424 - val_loss: 0.6709 - val_accuracy: 0.5717 - val_my_auc: 0.6321\n",
            " epoch:2 auc: 0.6329\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.6024 - my_auc: 0.6239 - val_loss: 0.6621 - val_accuracy: 0.6181 - val_my_auc: 0.6494\n",
            " epoch:3 auc: 0.6509\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6777 - accuracy: 0.6263 - my_auc: 0.6345 - val_loss: 0.6555 - val_accuracy: 0.6326 - val_my_auc: 0.6628\n",
            " epoch:4 auc: 0.6635\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6314 - accuracy: 0.6283 - my_auc: 0.6774 - val_loss: 0.6525 - val_accuracy: 0.6578 - val_my_auc: 0.6675\n",
            " epoch:5 auc: 0.6671\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6216 - accuracy: 0.6379 - my_auc: 0.6739 - val_loss: 0.6475 - val_accuracy: 0.6520 - val_my_auc: 0.6773\n",
            " epoch:6 auc: 0.6769\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6355 - accuracy: 0.6378 - my_auc: 0.6560 - val_loss: 0.6452 - val_accuracy: 0.6540 - val_my_auc: 0.6809\n",
            " epoch:7 auc: 0.6806\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6478 - accuracy: 0.6377 - my_auc: 0.6743 - val_loss: 0.6427 - val_accuracy: 0.6683 - val_my_auc: 0.6838\n",
            " epoch:8 auc: 0.6843\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6591 - accuracy: 0.6376 - my_auc: 0.6793 - val_loss: 0.6421 - val_accuracy: 0.6736 - val_my_auc: 0.6847\n",
            " epoch:9 auc: 0.6842\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6098 - accuracy: 0.6496 - my_auc: 0.6887 - val_loss: 0.6402 - val_accuracy: 0.6619 - val_my_auc: 0.6874\n",
            " epoch:10 auc: 0.6872\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 0.6493 - my_auc: 0.7064 - val_loss: 0.6382 - val_accuracy: 0.6572 - val_my_auc: 0.6901\n",
            " epoch:11 auc: 0.6902\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6490 - accuracy: 0.6330 - my_auc: 0.6950 - val_loss: 0.6395 - val_accuracy: 0.6755 - val_my_auc: 0.6889\n",
            " epoch:12 auc: 0.6887\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6535 - accuracy: 0.6382 - my_auc: 0.6549 - val_loss: 0.6394 - val_accuracy: 0.6701 - val_my_auc: 0.6889\n",
            " epoch:13 auc: 0.6883\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6669 - accuracy: 0.6328 - my_auc: 0.6636 - val_loss: 0.6381 - val_accuracy: 0.6559 - val_my_auc: 0.6907\n",
            " epoch:14 auc: 0.6906\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6578 - accuracy: 0.6235 - my_auc: 0.6835 - val_loss: 0.6366 - val_accuracy: 0.6626 - val_my_auc: 0.6940\n",
            " epoch:15 auc: 0.6942\n",
            "Epoch 17/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6287 - accuracy: 0.6392 - my_auc: 0.6844 - val_loss: 0.6367 - val_accuracy: 0.6591 - val_my_auc: 0.6938\n",
            " epoch:16 auc: 0.6934\n",
            "Epoch 18/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6118 - accuracy: 0.6459 - my_auc: 0.6868 - val_loss: 0.6356 - val_accuracy: 0.6520 - val_my_auc: 0.6948\n",
            " epoch:17 auc: 0.6948\n",
            "Epoch 19/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6451 - accuracy: 0.6246 - my_auc: 0.6935 - val_loss: 0.6358 - val_accuracy: 0.6602 - val_my_auc: 0.6953\n",
            " epoch:18 auc: 0.6952\n",
            "Epoch 20/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6029 - accuracy: 0.6393 - my_auc: 0.7003 - val_loss: 0.6345 - val_accuracy: 0.6433 - val_my_auc: 0.6953\n",
            " epoch:19 auc: 0.6957\n",
            "Epoch 21/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6065 - accuracy: 0.6294 - my_auc: 0.6941 - val_loss: 0.6340 - val_accuracy: 0.6428 - val_my_auc: 0.6968\n",
            " epoch:20 auc: 0.6967\n",
            "Epoch 22/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6380 - accuracy: 0.6149 - my_auc: 0.7047 - val_loss: 0.6336 - val_accuracy: 0.6516 - val_my_auc: 0.6978\n",
            " epoch:21 auc: 0.6977\n",
            "Epoch 23/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6009 - accuracy: 0.6332 - my_auc: 0.7126 - val_loss: 0.6328 - val_accuracy: 0.6475 - val_my_auc: 0.6993\n",
            " epoch:22 auc: 0.6990\n",
            "Epoch 24/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6139 - accuracy: 0.6317 - my_auc: 0.7195 - val_loss: 0.6328 - val_accuracy: 0.6514 - val_my_auc: 0.6989\n",
            " epoch:23 auc: 0.6991\n",
            "Epoch 25/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6069 - accuracy: 0.6335 - my_auc: 0.7064 - val_loss: 0.6317 - val_accuracy: 0.6409 - val_my_auc: 0.7003\n",
            " epoch:24 auc: 0.7008\n",
            "Epoch 26/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.5944 - accuracy: 0.6209 - my_auc: 0.7031 - val_loss: 0.6312 - val_accuracy: 0.6445 - val_my_auc: 0.7020\n",
            " epoch:25 auc: 0.7021\n",
            "Epoch 27/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6184 - accuracy: 0.6252 - my_auc: 0.6942 - val_loss: 0.6309 - val_accuracy: 0.6393 - val_my_auc: 0.7025\n",
            " epoch:26 auc: 0.7027\n",
            "Epoch 28/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.5850 - accuracy: 0.6229 - my_auc: 0.7046 - val_loss: 0.6307 - val_accuracy: 0.6290 - val_my_auc: 0.7014\n",
            " epoch:27 auc: 0.7020\n",
            "Epoch 29/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6350 - accuracy: 0.6088 - my_auc: 0.7049 - val_loss: 0.6310 - val_accuracy: 0.6417 - val_my_auc: 0.7028\n",
            " epoch:28 auc: 0.7026\n",
            "Epoch 30/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6056 - accuracy: 0.6237 - my_auc: 0.6744 - val_loss: 0.6307 - val_accuracy: 0.6306 - val_my_auc: 0.7028\n",
            " epoch:29 auc: 0.7030\n",
            "AUC:  0.6613\n",
            "iterate_num:  2 \n",
            " sum of test_y: 122.0\n",
            "train_x_shape (32180, 14)\n",
            "Epoch 1/30\n",
            "63/63 [==============================] - 3s 25ms/step - loss: 0.6915 - accuracy: 0.2894 - my_auc: 0.5100 - val_loss: 0.6923 - val_accuracy: 0.1689 - val_my_auc: 0.5436\n",
            " epoch:0 auc: 0.5425\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6974 - accuracy: 0.4558 - my_auc: 0.5612 - val_loss: 0.6863 - val_accuracy: 0.2719 - val_my_auc: 0.5806\n",
            " epoch:1 auc: 0.5806\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6733 - accuracy: 0.5685 - my_auc: 0.6059 - val_loss: 0.6809 - val_accuracy: 0.4497 - val_my_auc: 0.6099\n",
            " epoch:2 auc: 0.6114\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6699 - accuracy: 0.6290 - my_auc: 0.5974 - val_loss: 0.6784 - val_accuracy: 0.5306 - val_my_auc: 0.6192\n",
            " epoch:3 auc: 0.6191\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6948 - accuracy: 0.6176 - my_auc: 0.6212 - val_loss: 0.6771 - val_accuracy: 0.6120 - val_my_auc: 0.6176\n",
            " epoch:4 auc: 0.6173\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6716 - accuracy: 0.6717 - my_auc: 0.6480 - val_loss: 0.6783 - val_accuracy: 0.6266 - val_my_auc: 0.6185\n",
            " epoch:5 auc: 0.6179\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6688 - accuracy: 0.6516 - my_auc: 0.6648 - val_loss: 0.6786 - val_accuracy: 0.6443 - val_my_auc: 0.6225\n",
            " epoch:6 auc: 0.6229\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6359 - accuracy: 0.6699 - my_auc: 0.6331 - val_loss: 0.6803 - val_accuracy: 0.6399 - val_my_auc: 0.6237\n",
            " epoch:7 auc: 0.6234\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.6500 - my_auc: 0.6341 - val_loss: 0.6821 - val_accuracy: 0.6353 - val_my_auc: 0.6228\n",
            " epoch:8 auc: 0.6235\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6431 - accuracy: 0.6514 - my_auc: 0.6823 - val_loss: 0.6834 - val_accuracy: 0.6376 - val_my_auc: 0.6252\n",
            " epoch:9 auc: 0.6250\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6432 - accuracy: 0.6498 - my_auc: 0.6774 - val_loss: 0.6839 - val_accuracy: 0.6299 - val_my_auc: 0.6262\n",
            " epoch:10 auc: 0.6268\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6530 - accuracy: 0.6381 - my_auc: 0.6760 - val_loss: 0.6856 - val_accuracy: 0.6352 - val_my_auc: 0.6270\n",
            " epoch:11 auc: 0.6267\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6943 - accuracy: 0.6415 - my_auc: 0.6525 - val_loss: 0.6861 - val_accuracy: 0.6396 - val_my_auc: 0.6273\n",
            " epoch:12 auc: 0.6269\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6507 - accuracy: 0.6570 - my_auc: 0.6716 - val_loss: 0.6868 - val_accuracy: 0.6352 - val_my_auc: 0.6259\n",
            " epoch:13 auc: 0.6261\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6378 - accuracy: 0.6542 - my_auc: 0.6940 - val_loss: 0.6892 - val_accuracy: 0.6334 - val_my_auc: 0.6229\n",
            " epoch:14 auc: 0.6234\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6984 - accuracy: 0.6322 - my_auc: 0.6847 - val_loss: 0.6909 - val_accuracy: 0.6566 - val_my_auc: 0.6253\n",
            " epoch:15 auc: 0.6251\n",
            "AUC:  0.6749\n",
            "iterate_num:  3 \n",
            " sum of test_y: 122.0\n",
            "train_x_shape (32180, 14)\n",
            "Epoch 1/30\n",
            "63/63 [==============================] - 3s 24ms/step - loss: 0.8430 - accuracy: 0.9502 - my_auc: 0.5522 - val_loss: 0.7213 - val_accuracy: 0.8945 - val_my_auc: 0.5166\n",
            " epoch:0 auc: 0.5168\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.7125 - accuracy: 0.8418 - my_auc: 0.5977 - val_loss: 0.6927 - val_accuracy: 0.6265 - val_my_auc: 0.5422\n",
            " epoch:1 auc: 0.5405\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.7025 - accuracy: 0.7290 - my_auc: 0.6349 - val_loss: 0.6824 - val_accuracy: 0.4887 - val_my_auc: 0.5661\n",
            " epoch:2 auc: 0.5642\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6998 - accuracy: 0.6607 - my_auc: 0.6138 - val_loss: 0.6767 - val_accuracy: 0.4444 - val_my_auc: 0.5833\n",
            " epoch:3 auc: 0.5824\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6511 - accuracy: 0.6298 - my_auc: 0.6450 - val_loss: 0.6725 - val_accuracy: 0.4729 - val_my_auc: 0.5946\n",
            " epoch:4 auc: 0.5948\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6490 - accuracy: 0.6264 - my_auc: 0.6546 - val_loss: 0.6700 - val_accuracy: 0.5150 - val_my_auc: 0.6040\n",
            " epoch:5 auc: 0.6037\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6749 - accuracy: 0.6275 - my_auc: 0.6379 - val_loss: 0.6672 - val_accuracy: 0.5385 - val_my_auc: 0.6132\n",
            " epoch:6 auc: 0.6133\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6239 - accuracy: 0.6318 - my_auc: 0.6867 - val_loss: 0.6665 - val_accuracy: 0.5588 - val_my_auc: 0.6190\n",
            " epoch:7 auc: 0.6182\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6562 - accuracy: 0.6207 - my_auc: 0.6919 - val_loss: 0.6674 - val_accuracy: 0.6035 - val_my_auc: 0.6226\n",
            " epoch:8 auc: 0.6226\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.6342 - accuracy: 0.6555 - my_auc: 0.6908 - val_loss: 0.6658 - val_accuracy: 0.5897 - val_my_auc: 0.6277\n",
            " epoch:9 auc: 0.6278\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.5950 - accuracy: 0.6463 - my_auc: 0.7155 - val_loss: 0.6650 - val_accuracy: 0.5812 - val_my_auc: 0.6314\n",
            " epoch:10 auc: 0.6315\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.6289 - accuracy: 0.6305 - my_auc: 0.7215 - val_loss: 0.6646 - val_accuracy: 0.5758 - val_my_auc: 0.6338\n",
            " epoch:11 auc: 0.6339\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6170 - accuracy: 0.6299 - my_auc: 0.7101 - val_loss: 0.6643 - val_accuracy: 0.5828 - val_my_auc: 0.6354\n",
            " epoch:12 auc: 0.6351\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6079 - accuracy: 0.6330 - my_auc: 0.7032 - val_loss: 0.6636 - val_accuracy: 0.5785 - val_my_auc: 0.6374\n",
            " epoch:13 auc: 0.6371\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6228 - accuracy: 0.6241 - my_auc: 0.6758 - val_loss: 0.6663 - val_accuracy: 0.5945 - val_my_auc: 0.6370\n",
            " epoch:14 auc: 0.6370\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.6288 - accuracy: 0.6280 - my_auc: 0.6912 - val_loss: 0.6659 - val_accuracy: 0.5889 - val_my_auc: 0.6380\n",
            " epoch:15 auc: 0.6382\n",
            "Epoch 17/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6312 - accuracy: 0.6236 - my_auc: 0.6928 - val_loss: 0.6658 - val_accuracy: 0.5854 - val_my_auc: 0.6386\n",
            " epoch:16 auc: 0.6386\n",
            "Epoch 18/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6722 - accuracy: 0.6146 - my_auc: 0.6890 - val_loss: 0.6665 - val_accuracy: 0.5823 - val_my_auc: 0.6380\n",
            " epoch:17 auc: 0.6378\n",
            "Epoch 19/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6219 - accuracy: 0.6276 - my_auc: 0.7046 - val_loss: 0.6681 - val_accuracy: 0.5831 - val_my_auc: 0.6385\n",
            " epoch:18 auc: 0.6382\n",
            "Epoch 20/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.5990 - accuracy: 0.6228 - my_auc: 0.7249 - val_loss: 0.6689 - val_accuracy: 0.5942 - val_my_auc: 0.6410\n",
            " epoch:19 auc: 0.6408\n",
            "Epoch 21/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6194 - accuracy: 0.6280 - my_auc: 0.7179 - val_loss: 0.6699 - val_accuracy: 0.5991 - val_my_auc: 0.6407\n",
            " epoch:20 auc: 0.6408\n",
            "Epoch 22/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6063 - accuracy: 0.6318 - my_auc: 0.7357 - val_loss: 0.6714 - val_accuracy: 0.6076 - val_my_auc: 0.6400\n",
            " epoch:21 auc: 0.6396\n",
            "Epoch 23/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6033 - accuracy: 0.6387 - my_auc: 0.7420 - val_loss: 0.6703 - val_accuracy: 0.5905 - val_my_auc: 0.6407\n",
            " epoch:22 auc: 0.6402\n",
            "AUC:  0.6553\n",
            "iterate_num:  4 \n",
            " sum of test_y: 121.0\n",
            "train_x_shape (32181, 14)\n",
            "Epoch 1/30\n",
            "63/63 [==============================] - 3s 26ms/step - loss: 0.7374 - accuracy: 0.4365 - my_auc: 0.4429 - val_loss: 0.6904 - val_accuracy: 0.1454 - val_my_auc: 0.5606\n",
            " epoch:0 auc: 0.5621\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.6821 - accuracy: 0.5767 - my_auc: 0.5075 - val_loss: 0.6827 - val_accuracy: 0.2928 - val_my_auc: 0.6076\n",
            " epoch:1 auc: 0.6072\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6988 - accuracy: 0.6168 - my_auc: 0.5149 - val_loss: 0.6786 - val_accuracy: 0.4105 - val_my_auc: 0.6250\n",
            " epoch:2 auc: 0.6225\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6658 - accuracy: 0.6277 - my_auc: 0.6067 - val_loss: 0.6750 - val_accuracy: 0.4733 - val_my_auc: 0.6216\n",
            " epoch:3 auc: 0.6224\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6904 - accuracy: 0.6016 - my_auc: 0.5911 - val_loss: 0.6691 - val_accuracy: 0.5232 - val_my_auc: 0.6360\n",
            " epoch:4 auc: 0.6342\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6567 - accuracy: 0.6266 - my_auc: 0.5940 - val_loss: 0.6638 - val_accuracy: 0.5308 - val_my_auc: 0.6409\n",
            " epoch:5 auc: 0.6403\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6726 - accuracy: 0.5853 - my_auc: 0.6341 - val_loss: 0.6589 - val_accuracy: 0.5410 - val_my_auc: 0.6429\n",
            " epoch:6 auc: 0.6442\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6162 - accuracy: 0.6092 - my_auc: 0.6374 - val_loss: 0.6552 - val_accuracy: 0.5267 - val_my_auc: 0.6471\n",
            " epoch:7 auc: 0.6476\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6390 - accuracy: 0.5806 - my_auc: 0.6216 - val_loss: 0.6516 - val_accuracy: 0.5322 - val_my_auc: 0.6528\n",
            " epoch:8 auc: 0.6532\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6640 - accuracy: 0.5532 - my_auc: 0.6173 - val_loss: 0.6481 - val_accuracy: 0.5466 - val_my_auc: 0.6595\n",
            " epoch:9 auc: 0.6580\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6954 - accuracy: 0.5598 - my_auc: 0.6069 - val_loss: 0.6442 - val_accuracy: 0.5573 - val_my_auc: 0.6676\n",
            " epoch:10 auc: 0.6657\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6682 - accuracy: 0.5733 - my_auc: 0.6253 - val_loss: 0.6424 - val_accuracy: 0.5539 - val_my_auc: 0.6668\n",
            " epoch:11 auc: 0.6677\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6409 - accuracy: 0.5780 - my_auc: 0.6531 - val_loss: 0.6402 - val_accuracy: 0.5520 - val_my_auc: 0.6712\n",
            " epoch:12 auc: 0.6709\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6568 - accuracy: 0.5568 - my_auc: 0.6312 - val_loss: 0.6397 - val_accuracy: 0.5357 - val_my_auc: 0.6712\n",
            " epoch:13 auc: 0.6712\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6709 - accuracy: 0.5385 - my_auc: 0.6575 - val_loss: 0.6371 - val_accuracy: 0.5592 - val_my_auc: 0.6750\n",
            " epoch:14 auc: 0.6751\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6548 - accuracy: 0.5663 - my_auc: 0.6773 - val_loss: 0.6380 - val_accuracy: 0.5676 - val_my_auc: 0.6729\n",
            " epoch:15 auc: 0.6723\n",
            "Epoch 17/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6445 - accuracy: 0.5775 - my_auc: 0.6342 - val_loss: 0.6354 - val_accuracy: 0.5830 - val_my_auc: 0.6768\n",
            " epoch:16 auc: 0.6767\n",
            "Epoch 18/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6482 - accuracy: 0.5890 - my_auc: 0.6712 - val_loss: 0.6351 - val_accuracy: 0.5751 - val_my_auc: 0.6755\n",
            " epoch:17 auc: 0.6753\n",
            "Epoch 19/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.5877 - my_auc: 0.6664 - val_loss: 0.6359 - val_accuracy: 0.5667 - val_my_auc: 0.6730\n",
            " epoch:18 auc: 0.6727\n",
            "Epoch 20/30\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.6643 - accuracy: 0.5648 - my_auc: 0.6372 - val_loss: 0.6358 - val_accuracy: 0.5633 - val_my_auc: 0.6723\n",
            " epoch:19 auc: 0.6721\n",
            "AUC:  0.6669\n",
            "avg_AUC :  0.664600909121805\n",
            "avg_AUC_2 :  0.6647527982712916\n",
            "('perf', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue'])\n",
            "0.664600909121805\n",
            "['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3']\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 17)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 29ms/step - loss: 0.7350 - accuracy: 0.5573 - my_auc: 0.4976 - val_loss: 0.6910 - val_accuracy: 0.1314 - val_my_auc: 0.5644\n",
            " epoch:0 auc: 0.5691\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6738 - accuracy: 0.6327 - my_auc: 0.5327 - val_loss: 0.6845 - val_accuracy: 0.1840 - val_my_auc: 0.5969\n",
            " epoch:1 auc: 0.5975\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6517 - accuracy: 0.6523 - my_auc: 0.6004 - val_loss: 0.6747 - val_accuracy: 0.3059 - val_my_auc: 0.6125\n",
            " epoch:2 auc: 0.6143\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6393 - accuracy: 0.6897 - my_auc: 0.6103 - val_loss: 0.6687 - val_accuracy: 0.4160 - val_my_auc: 0.6228\n",
            " epoch:3 auc: 0.6230\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6275 - accuracy: 0.6904 - my_auc: 0.6808 - val_loss: 0.6644 - val_accuracy: 0.5386 - val_my_auc: 0.6295\n",
            " epoch:4 auc: 0.6291\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.5922 - accuracy: 0.7186 - my_auc: 0.6943 - val_loss: 0.6621 - val_accuracy: 0.5927 - val_my_auc: 0.6350\n",
            " epoch:5 auc: 0.6349\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6382 - accuracy: 0.6897 - my_auc: 0.6713 - val_loss: 0.6622 - val_accuracy: 0.6346 - val_my_auc: 0.6399\n",
            " epoch:6 auc: 0.6403\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6389 - accuracy: 0.6881 - my_auc: 0.6788 - val_loss: 0.6634 - val_accuracy: 0.6453 - val_my_auc: 0.6449\n",
            " epoch:7 auc: 0.6449\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.5913 - accuracy: 0.6915 - my_auc: 0.7131 - val_loss: 0.6631 - val_accuracy: 0.6258 - val_my_auc: 0.6482\n",
            " epoch:8 auc: 0.6481\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6340 - accuracy: 0.6501 - my_auc: 0.7019 - val_loss: 0.6655 - val_accuracy: 0.6326 - val_my_auc: 0.6501\n",
            " epoch:9 auc: 0.6502\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6087 - accuracy: 0.6436 - my_auc: 0.6972 - val_loss: 0.6671 - val_accuracy: 0.6504 - val_my_auc: 0.6529\n",
            " epoch:10 auc: 0.6525\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6373 - accuracy: 0.6440 - my_auc: 0.6788 - val_loss: 0.6672 - val_accuracy: 0.6592 - val_my_auc: 0.6549\n",
            " epoch:11 auc: 0.6551\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6236 - accuracy: 0.6535 - my_auc: 0.6998 - val_loss: 0.6676 - val_accuracy: 0.6466 - val_my_auc: 0.6561\n",
            " epoch:12 auc: 0.6559\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6347 - accuracy: 0.6343 - my_auc: 0.7124 - val_loss: 0.6697 - val_accuracy: 0.6615 - val_my_auc: 0.6575\n",
            " epoch:13 auc: 0.6579\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6250 - accuracy: 0.6502 - my_auc: 0.7086 - val_loss: 0.6711 - val_accuracy: 0.6643 - val_my_auc: 0.6584\n",
            " epoch:14 auc: 0.6579\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6210 - accuracy: 0.6480 - my_auc: 0.7168 - val_loss: 0.6720 - val_accuracy: 0.6609 - val_my_auc: 0.6581\n",
            " epoch:15 auc: 0.6579\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6050 - accuracy: 0.6458 - my_auc: 0.7151 - val_loss: 0.6752 - val_accuracy: 0.6735 - val_my_auc: 0.6587\n",
            " epoch:16 auc: 0.6590\n",
            "Epoch 18/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6120 - accuracy: 0.6502 - my_auc: 0.7246 - val_loss: 0.6757 - val_accuracy: 0.6716 - val_my_auc: 0.6591\n",
            " epoch:17 auc: 0.6596\n",
            "Epoch 19/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.5753 - accuracy: 0.6601 - my_auc: 0.7527 - val_loss: 0.6753 - val_accuracy: 0.6665 - val_my_auc: 0.6614\n",
            " epoch:18 auc: 0.6614\n",
            "Epoch 20/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6234 - accuracy: 0.6418 - my_auc: 0.7025 - val_loss: 0.6759 - val_accuracy: 0.6697 - val_my_auc: 0.6626\n",
            " epoch:19 auc: 0.6623\n",
            "Epoch 21/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6090 - accuracy: 0.6504 - my_auc: 0.7043 - val_loss: 0.6770 - val_accuracy: 0.6796 - val_my_auc: 0.6658\n",
            " epoch:20 auc: 0.6648\n",
            "Epoch 22/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5976 - accuracy: 0.6527 - my_auc: 0.7053 - val_loss: 0.6756 - val_accuracy: 0.6735 - val_my_auc: 0.6670\n",
            " epoch:21 auc: 0.6665\n",
            "Epoch 23/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6016 - accuracy: 0.6502 - my_auc: 0.7391 - val_loss: 0.6762 - val_accuracy: 0.6675 - val_my_auc: 0.6667\n",
            " epoch:22 auc: 0.6662\n",
            "Epoch 24/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5716 - accuracy: 0.6464 - my_auc: 0.7328 - val_loss: 0.6768 - val_accuracy: 0.6682 - val_my_auc: 0.6671\n",
            " epoch:23 auc: 0.6673\n",
            "Epoch 25/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6093 - accuracy: 0.6441 - my_auc: 0.7340 - val_loss: 0.6781 - val_accuracy: 0.6723 - val_my_auc: 0.6681\n",
            " epoch:24 auc: 0.6682\n",
            "Epoch 26/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6182 - accuracy: 0.6383 - my_auc: 0.7183 - val_loss: 0.6805 - val_accuracy: 0.6899 - val_my_auc: 0.6693\n",
            " epoch:25 auc: 0.6695\n",
            "Epoch 27/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6216 - accuracy: 0.6522 - my_auc: 0.7253 - val_loss: 0.6782 - val_accuracy: 0.6808 - val_my_auc: 0.6720\n",
            " epoch:26 auc: 0.6720\n",
            "Epoch 28/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5797 - accuracy: 0.6560 - my_auc: 0.7549 - val_loss: 0.6808 - val_accuracy: 0.6858 - val_my_auc: 0.6728\n",
            " epoch:27 auc: 0.6726\n",
            "Epoch 29/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5943 - accuracy: 0.6614 - my_auc: 0.7142 - val_loss: 0.6810 - val_accuracy: 0.6843 - val_my_auc: 0.6730\n",
            " epoch:28 auc: 0.6732\n",
            "Epoch 30/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5968 - accuracy: 0.6540 - my_auc: 0.7350 - val_loss: 0.6802 - val_accuracy: 0.6846 - val_my_auc: 0.6749\n",
            " epoch:29 auc: 0.6754\n",
            "AUC:  0.6902\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 17)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 25ms/step - loss: 0.6803 - accuracy: 0.6935 - my_auc: 0.5449 - val_loss: 0.7026 - val_accuracy: 0.9043 - val_my_auc: 0.4855\n",
            " epoch:0 auc: 0.4839\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6903 - accuracy: 0.6441 - my_auc: 0.5746 - val_loss: 0.6909 - val_accuracy: 0.7512 - val_my_auc: 0.5514\n",
            " epoch:1 auc: 0.5508\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.7088 - accuracy: 0.6240 - my_auc: 0.5983 - val_loss: 0.6851 - val_accuracy: 0.7188 - val_my_auc: 0.5910\n",
            " epoch:2 auc: 0.5931\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.7090 - accuracy: 0.6594 - my_auc: 0.6030 - val_loss: 0.6787 - val_accuracy: 0.6732 - val_my_auc: 0.6245\n",
            " epoch:3 auc: 0.6229\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6716 - accuracy: 0.6718 - my_auc: 0.6412 - val_loss: 0.6716 - val_accuracy: 0.6622 - val_my_auc: 0.6475\n",
            " epoch:4 auc: 0.6477\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6455 - accuracy: 0.6700 - my_auc: 0.7052 - val_loss: 0.6647 - val_accuracy: 0.6747 - val_my_auc: 0.6672\n",
            " epoch:5 auc: 0.6675\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6742 - accuracy: 0.6618 - my_auc: 0.6553 - val_loss: 0.6581 - val_accuracy: 0.6874 - val_my_auc: 0.6840\n",
            " epoch:6 auc: 0.6836\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6425 - accuracy: 0.6831 - my_auc: 0.6630 - val_loss: 0.6512 - val_accuracy: 0.6399 - val_my_auc: 0.6931\n",
            " epoch:7 auc: 0.6930\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6366 - accuracy: 0.6555 - my_auc: 0.6775 - val_loss: 0.6473 - val_accuracy: 0.6672 - val_my_auc: 0.6991\n",
            " epoch:8 auc: 0.6989\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6551 - accuracy: 0.6625 - my_auc: 0.6844 - val_loss: 0.6428 - val_accuracy: 0.6656 - val_my_auc: 0.7061\n",
            " epoch:9 auc: 0.7050\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6429 - accuracy: 0.6654 - my_auc: 0.6714 - val_loss: 0.6396 - val_accuracy: 0.6545 - val_my_auc: 0.7080\n",
            " epoch:10 auc: 0.7080\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6206 - accuracy: 0.6620 - my_auc: 0.6969 - val_loss: 0.6383 - val_accuracy: 0.6505 - val_my_auc: 0.7072\n",
            " epoch:11 auc: 0.7077\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6374 - accuracy: 0.6570 - my_auc: 0.6882 - val_loss: 0.6354 - val_accuracy: 0.6685 - val_my_auc: 0.7119\n",
            " epoch:12 auc: 0.7120\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6313 - accuracy: 0.6644 - my_auc: 0.7214 - val_loss: 0.6345 - val_accuracy: 0.6732 - val_my_auc: 0.7135\n",
            " epoch:13 auc: 0.7138\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6761 - accuracy: 0.6520 - my_auc: 0.6764 - val_loss: 0.6330 - val_accuracy: 0.6590 - val_my_auc: 0.7152\n",
            " epoch:14 auc: 0.7146\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6249 - accuracy: 0.6545 - my_auc: 0.7022 - val_loss: 0.6330 - val_accuracy: 0.6358 - val_my_auc: 0.7125\n",
            " epoch:15 auc: 0.7129\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6457 - accuracy: 0.6319 - my_auc: 0.7068 - val_loss: 0.6323 - val_accuracy: 0.6615 - val_my_auc: 0.7140\n",
            " epoch:16 auc: 0.7142\n",
            "Epoch 18/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6426 - accuracy: 0.6550 - my_auc: 0.6966 - val_loss: 0.6311 - val_accuracy: 0.6745 - val_my_auc: 0.7163\n",
            " epoch:17 auc: 0.7168\n",
            "Epoch 19/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6588 - accuracy: 0.6611 - my_auc: 0.6912 - val_loss: 0.6310 - val_accuracy: 0.6725 - val_my_auc: 0.7174\n",
            " epoch:18 auc: 0.7172\n",
            "Epoch 20/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6198 - accuracy: 0.6626 - my_auc: 0.7247 - val_loss: 0.6307 - val_accuracy: 0.6771 - val_my_auc: 0.7183\n",
            " epoch:19 auc: 0.7188\n",
            "Epoch 21/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6142 - accuracy: 0.6763 - my_auc: 0.7098 - val_loss: 0.6302 - val_accuracy: 0.6745 - val_my_auc: 0.7198\n",
            " epoch:20 auc: 0.7197\n",
            "Epoch 22/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6158 - accuracy: 0.6696 - my_auc: 0.7068 - val_loss: 0.6299 - val_accuracy: 0.6700 - val_my_auc: 0.7204\n",
            " epoch:21 auc: 0.7203\n",
            "Epoch 23/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6141 - accuracy: 0.6717 - my_auc: 0.7041 - val_loss: 0.6289 - val_accuracy: 0.6650 - val_my_auc: 0.7213\n",
            " epoch:22 auc: 0.7216\n",
            "Epoch 24/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6170 - accuracy: 0.6618 - my_auc: 0.7312 - val_loss: 0.6277 - val_accuracy: 0.6619 - val_my_auc: 0.7224\n",
            " epoch:23 auc: 0.7226\n",
            "Epoch 25/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5927 - accuracy: 0.6658 - my_auc: 0.7209 - val_loss: 0.6264 - val_accuracy: 0.6559 - val_my_auc: 0.7238\n",
            " epoch:24 auc: 0.7237\n",
            "Epoch 26/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6224 - accuracy: 0.6522 - my_auc: 0.7263 - val_loss: 0.6280 - val_accuracy: 0.6909 - val_my_auc: 0.7244\n",
            " epoch:25 auc: 0.7244\n",
            "Epoch 27/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.5919 - accuracy: 0.6841 - my_auc: 0.7166 - val_loss: 0.6272 - val_accuracy: 0.6602 - val_my_auc: 0.7225\n",
            " epoch:26 auc: 0.7227\n",
            "Epoch 28/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6112 - accuracy: 0.6584 - my_auc: 0.7087 - val_loss: 0.6270 - val_accuracy: 0.6593 - val_my_auc: 0.7225\n",
            " epoch:27 auc: 0.7223\n",
            "Epoch 29/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6180 - accuracy: 0.6566 - my_auc: 0.7012 - val_loss: 0.6281 - val_accuracy: 0.6690 - val_my_auc: 0.7211\n",
            " epoch:28 auc: 0.7215\n",
            "AUC:  0.6425\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 17)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 26ms/step - loss: 0.7139 - accuracy: 0.2949 - my_auc: 0.5314 - val_loss: 0.7102 - val_accuracy: 0.9623 - val_my_auc: 0.5611\n",
            " epoch:0 auc: 0.5650\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6862 - accuracy: 0.4863 - my_auc: 0.5979 - val_loss: 0.6991 - val_accuracy: 0.9170 - val_my_auc: 0.5849\n",
            " epoch:1 auc: 0.5825\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6665 - accuracy: 0.5807 - my_auc: 0.6308 - val_loss: 0.6885 - val_accuracy: 0.8780 - val_my_auc: 0.6088\n",
            " epoch:2 auc: 0.6089\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6697 - accuracy: 0.6575 - my_auc: 0.6458 - val_loss: 0.6764 - val_accuracy: 0.8208 - val_my_auc: 0.6352\n",
            " epoch:3 auc: 0.6356\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6873 - accuracy: 0.6645 - my_auc: 0.6275 - val_loss: 0.6705 - val_accuracy: 0.7787 - val_my_auc: 0.6451\n",
            " epoch:4 auc: 0.6453\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6635 - accuracy: 0.6938 - my_auc: 0.6658 - val_loss: 0.6672 - val_accuracy: 0.7520 - val_my_auc: 0.6483\n",
            " epoch:5 auc: 0.6482\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6488 - accuracy: 0.6959 - my_auc: 0.6839 - val_loss: 0.6632 - val_accuracy: 0.7290 - val_my_auc: 0.6535\n",
            " epoch:6 auc: 0.6532\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5950 - accuracy: 0.7154 - my_auc: 0.7079 - val_loss: 0.6616 - val_accuracy: 0.7144 - val_my_auc: 0.6539\n",
            " epoch:7 auc: 0.6540\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6095 - accuracy: 0.7115 - my_auc: 0.7230 - val_loss: 0.6585 - val_accuracy: 0.7128 - val_my_auc: 0.6580\n",
            " epoch:8 auc: 0.6579\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6653 - accuracy: 0.7037 - my_auc: 0.6910 - val_loss: 0.6564 - val_accuracy: 0.7103 - val_my_auc: 0.6626\n",
            " epoch:9 auc: 0.6621\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6348 - accuracy: 0.7059 - my_auc: 0.6967 - val_loss: 0.6548 - val_accuracy: 0.6925 - val_my_auc: 0.6653\n",
            " epoch:10 auc: 0.6649\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.5983 - accuracy: 0.7040 - my_auc: 0.7134 - val_loss: 0.6533 - val_accuracy: 0.6862 - val_my_auc: 0.6679\n",
            " epoch:11 auc: 0.6685\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6317 - accuracy: 0.6909 - my_auc: 0.7180 - val_loss: 0.6533 - val_accuracy: 0.6939 - val_my_auc: 0.6714\n",
            " epoch:12 auc: 0.6709\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6347 - accuracy: 0.6847 - my_auc: 0.7152 - val_loss: 0.6532 - val_accuracy: 0.7030 - val_my_auc: 0.6744\n",
            " epoch:13 auc: 0.6740\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6132 - accuracy: 0.7065 - my_auc: 0.7297 - val_loss: 0.6527 - val_accuracy: 0.6922 - val_my_auc: 0.6750\n",
            " epoch:14 auc: 0.6752\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.6970 - my_auc: 0.7466 - val_loss: 0.6519 - val_accuracy: 0.6795 - val_my_auc: 0.6759\n",
            " epoch:15 auc: 0.6760\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6496 - accuracy: 0.6790 - my_auc: 0.7120 - val_loss: 0.6546 - val_accuracy: 0.6891 - val_my_auc: 0.6749\n",
            " epoch:16 auc: 0.6752\n",
            "Epoch 18/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5965 - accuracy: 0.6963 - my_auc: 0.7428 - val_loss: 0.6556 - val_accuracy: 0.6916 - val_my_auc: 0.6749\n",
            " epoch:17 auc: 0.6751\n",
            "Epoch 19/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6336 - accuracy: 0.6910 - my_auc: 0.7009 - val_loss: 0.6580 - val_accuracy: 0.7049 - val_my_auc: 0.6758\n",
            " epoch:18 auc: 0.6756\n",
            "AUC:  0.6601\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "train_x_shape (27367, 17)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 2s 25ms/step - loss: 0.7774 - accuracy: 0.3239 - my_auc: 0.5084 - val_loss: 0.6947 - val_accuracy: 0.9412 - val_my_auc: 0.5744\n",
            " epoch:0 auc: 0.5774\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.7239 - accuracy: 0.5287 - my_auc: 0.5337 - val_loss: 0.6890 - val_accuracy: 0.8917 - val_my_auc: 0.5950\n",
            " epoch:1 auc: 0.5919\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.7074 - accuracy: 0.5905 - my_auc: 0.5569 - val_loss: 0.6861 - val_accuracy: 0.8404 - val_my_auc: 0.5948\n",
            " epoch:2 auc: 0.5947\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6997 - accuracy: 0.6321 - my_auc: 0.5926 - val_loss: 0.6827 - val_accuracy: 0.7844 - val_my_auc: 0.6055\n",
            " epoch:3 auc: 0.6056\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6658 - accuracy: 0.6627 - my_auc: 0.6238 - val_loss: 0.6788 - val_accuracy: 0.7246 - val_my_auc: 0.6128\n",
            " epoch:4 auc: 0.6138\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6513 - accuracy: 0.6482 - my_auc: 0.6148 - val_loss: 0.6736 - val_accuracy: 0.6991 - val_my_auc: 0.6250\n",
            " epoch:5 auc: 0.6263\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6534 - accuracy: 0.6433 - my_auc: 0.6487 - val_loss: 0.6687 - val_accuracy: 0.6967 - val_my_auc: 0.6403\n",
            " epoch:6 auc: 0.6390\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6602 - accuracy: 0.6451 - my_auc: 0.6299 - val_loss: 0.6639 - val_accuracy: 0.7096 - val_my_auc: 0.6481\n",
            " epoch:7 auc: 0.6489\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6707 - accuracy: 0.6592 - my_auc: 0.6574 - val_loss: 0.6594 - val_accuracy: 0.6979 - val_my_auc: 0.6564\n",
            " epoch:8 auc: 0.6560\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6704 - accuracy: 0.6523 - my_auc: 0.6382 - val_loss: 0.6577 - val_accuracy: 0.7094 - val_my_auc: 0.6596\n",
            " epoch:9 auc: 0.6597\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.6749 - accuracy: 0.6563 - my_auc: 0.6464 - val_loss: 0.6562 - val_accuracy: 0.6964 - val_my_auc: 0.6601\n",
            " epoch:10 auc: 0.6599\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6751 - accuracy: 0.6432 - my_auc: 0.6725 - val_loss: 0.6548 - val_accuracy: 0.7001 - val_my_auc: 0.6628\n",
            " epoch:11 auc: 0.6631\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6306 - accuracy: 0.6666 - my_auc: 0.6896 - val_loss: 0.6540 - val_accuracy: 0.6982 - val_my_auc: 0.6624\n",
            " epoch:12 auc: 0.6619\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6172 - accuracy: 0.6736 - my_auc: 0.6907 - val_loss: 0.6514 - val_accuracy: 0.6922 - val_my_auc: 0.6649\n",
            " epoch:13 auc: 0.6653\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6199 - accuracy: 0.6671 - my_auc: 0.6658 - val_loss: 0.6518 - val_accuracy: 0.6773 - val_my_auc: 0.6640\n",
            " epoch:14 auc: 0.6647\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6680 - accuracy: 0.6418 - my_auc: 0.6720 - val_loss: 0.6507 - val_accuracy: 0.6739 - val_my_auc: 0.6649\n",
            " epoch:15 auc: 0.6649\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.6616 - accuracy: 0.6456 - my_auc: 0.6917 - val_loss: 0.6511 - val_accuracy: 0.6749 - val_my_auc: 0.6630\n",
            " epoch:16 auc: 0.6634\n",
            "AUC:  0.6831\n",
            "avg_AUC :  0.6689966807389838\n",
            "avg_AUC_2 :  0.6692205985304411\n",
            "('perf+spc', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3'])\n",
            "0.6689966807389838\n",
            "['Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis', 's1', 'pastavg3', 'cr3']\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 43)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 27ms/step - loss: 0.7065 - accuracy: 0.5927 - my_auc: 0.4970 - val_loss: 0.6912 - val_accuracy: 0.1115 - val_my_auc: 0.5753\n",
            " epoch:0 auc: 0.5835\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.7117 - accuracy: 0.6641 - my_auc: 0.5665 - val_loss: 0.6886 - val_accuracy: 0.4051 - val_my_auc: 0.6248\n",
            " epoch:1 auc: 0.6288\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6513 - accuracy: 0.7155 - my_auc: 0.6343 - val_loss: 0.6868 - val_accuracy: 0.3655 - val_my_auc: 0.6277\n",
            " epoch:2 auc: 0.6370\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6600 - accuracy: 0.7005 - my_auc: 0.7200 - val_loss: 0.6841 - val_accuracy: 0.4171 - val_my_auc: 0.6465\n",
            " epoch:3 auc: 0.6468\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6915 - accuracy: 0.6687 - my_auc: 0.6476 - val_loss: 0.6804 - val_accuracy: 0.5053 - val_my_auc: 0.6450\n",
            " epoch:4 auc: 0.6450\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6612 - accuracy: 0.6816 - my_auc: 0.6930 - val_loss: 0.6783 - val_accuracy: 0.5045 - val_my_auc: 0.6335\n",
            " epoch:5 auc: 0.6329\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 0.6270 - accuracy: 0.6804 - my_auc: 0.6920 - val_loss: 0.6746 - val_accuracy: 0.4980 - val_my_auc: 0.6363\n",
            " epoch:6 auc: 0.6352\n",
            "AUC:  0.6126\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 43)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 28ms/step - loss: 0.7125 - accuracy: 0.3176 - my_auc: 0.4734 - val_loss: 0.6908 - val_accuracy: 0.9344 - val_my_auc: 0.5930\n",
            " epoch:0 auc: 0.5880\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6520 - accuracy: 0.6236 - my_auc: 0.6199 - val_loss: 0.6893 - val_accuracy: 0.9247 - val_my_auc: 0.6032\n",
            " epoch:1 auc: 0.6040\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6793 - accuracy: 0.6092 - my_auc: 0.6084 - val_loss: 0.6867 - val_accuracy: 0.8372 - val_my_auc: 0.6196\n",
            " epoch:2 auc: 0.6165\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 0.6856 - accuracy: 0.6480 - my_auc: 0.6065 - val_loss: 0.6846 - val_accuracy: 0.7140 - val_my_auc: 0.6179\n",
            " epoch:3 auc: 0.6165\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6571 - accuracy: 0.6379 - my_auc: 0.6689 - val_loss: 0.6822 - val_accuracy: 0.6711 - val_my_auc: 0.6202\n",
            " epoch:4 auc: 0.6206\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6828 - accuracy: 0.6321 - my_auc: 0.6510 - val_loss: 0.6779 - val_accuracy: 0.6555 - val_my_auc: 0.6212\n",
            " epoch:5 auc: 0.6234\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6689 - accuracy: 0.6520 - my_auc: 0.6807 - val_loss: 0.6770 - val_accuracy: 0.6402 - val_my_auc: 0.6204\n",
            " epoch:6 auc: 0.6199\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6670 - accuracy: 0.6400 - my_auc: 0.7115 - val_loss: 0.6737 - val_accuracy: 0.6647 - val_my_auc: 0.6213\n",
            " epoch:7 auc: 0.6221\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6637 - accuracy: 0.6669 - my_auc: 0.6926 - val_loss: 0.6720 - val_accuracy: 0.6925 - val_my_auc: 0.6250\n",
            " epoch:8 auc: 0.6253\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6646 - accuracy: 0.6701 - my_auc: 0.6928 - val_loss: 0.6735 - val_accuracy: 0.7267 - val_my_auc: 0.6239\n",
            " epoch:9 auc: 0.6237\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6220 - accuracy: 0.6968 - my_auc: 0.7288 - val_loss: 0.6730 - val_accuracy: 0.6862 - val_my_auc: 0.6203\n",
            " epoch:10 auc: 0.6208\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6059 - accuracy: 0.6791 - my_auc: 0.7230 - val_loss: 0.6763 - val_accuracy: 0.6893 - val_my_auc: 0.6181\n",
            " epoch:11 auc: 0.6184\n",
            "AUC:  0.5849\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 43)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 27ms/step - loss: 0.7441 - accuracy: 0.3780 - my_auc: 0.4503 - val_loss: 0.6965 - val_accuracy: 0.0368 - val_my_auc: 0.4501\n",
            " epoch:0 auc: 0.4530\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6975 - accuracy: 0.6357 - my_auc: 0.5586 - val_loss: 0.6944 - val_accuracy: 0.1980 - val_my_auc: 0.5001\n",
            " epoch:1 auc: 0.4960\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6683 - accuracy: 0.6586 - my_auc: 0.5675 - val_loss: 0.6933 - val_accuracy: 0.2818 - val_my_auc: 0.5212\n",
            " epoch:2 auc: 0.5246\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6598 - accuracy: 0.6345 - my_auc: 0.6277 - val_loss: 0.6910 - val_accuracy: 0.3655 - val_my_auc: 0.5647\n",
            " epoch:3 auc: 0.5661\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6612 - accuracy: 0.6309 - my_auc: 0.6342 - val_loss: 0.6879 - val_accuracy: 0.4735 - val_my_auc: 0.5894\n",
            " epoch:4 auc: 0.5888\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6615 - accuracy: 0.6569 - my_auc: 0.6813 - val_loss: 0.6857 - val_accuracy: 0.4906 - val_my_auc: 0.6030\n",
            " epoch:5 auc: 0.6001\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6608 - accuracy: 0.6555 - my_auc: 0.6824 - val_loss: 0.6844 - val_accuracy: 0.5314 - val_my_auc: 0.6079\n",
            " epoch:6 auc: 0.6085\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6499 - accuracy: 0.6632 - my_auc: 0.6745 - val_loss: 0.6840 - val_accuracy: 0.5552 - val_my_auc: 0.6128\n",
            " epoch:7 auc: 0.6139\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 0.6651 - accuracy: 0.6524 - my_auc: 0.6617 - val_loss: 0.6850 - val_accuracy: 0.6361 - val_my_auc: 0.6225\n",
            " epoch:8 auc: 0.6218\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6733 - accuracy: 0.6868 - my_auc: 0.6612 - val_loss: 0.6870 - val_accuracy: 0.6415 - val_my_auc: 0.6264\n",
            " epoch:9 auc: 0.6264\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6237 - accuracy: 0.6921 - my_auc: 0.6785 - val_loss: 0.6897 - val_accuracy: 0.6276 - val_my_auc: 0.6269\n",
            " epoch:10 auc: 0.6284\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6302 - accuracy: 0.6673 - my_auc: 0.6966 - val_loss: 0.6914 - val_accuracy: 0.6388 - val_my_auc: 0.6277\n",
            " epoch:11 auc: 0.6282\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6508 - accuracy: 0.6592 - my_auc: 0.7109 - val_loss: 0.6968 - val_accuracy: 0.6725 - val_my_auc: 0.6293\n",
            " epoch:12 auc: 0.6295\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6412 - accuracy: 0.6888 - my_auc: 0.7019 - val_loss: 0.6984 - val_accuracy: 0.6413 - val_my_auc: 0.6264\n",
            " epoch:13 auc: 0.6264\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6277 - accuracy: 0.6621 - my_auc: 0.7194 - val_loss: 0.7022 - val_accuracy: 0.6825 - val_my_auc: 0.6261\n",
            " epoch:14 auc: 0.6266\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6005 - accuracy: 0.6810 - my_auc: 0.7444 - val_loss: 0.7038 - val_accuracy: 0.6757 - val_my_auc: 0.6236\n",
            " epoch:15 auc: 0.6235\n",
            "AUC:  0.5801\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "train_x_shape (27367, 43)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 28ms/step - loss: 0.7210 - accuracy: 0.8577 - my_auc: 0.4873 - val_loss: 0.6933 - val_accuracy: 0.0203 - val_my_auc: 0.5303\n",
            " epoch:0 auc: 0.5403\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.7253 - accuracy: 0.6375 - my_auc: 0.5412 - val_loss: 0.6921 - val_accuracy: 0.0851 - val_my_auc: 0.5569\n",
            " epoch:1 auc: 0.5640\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.7174 - accuracy: 0.6449 - my_auc: 0.5778 - val_loss: 0.6891 - val_accuracy: 0.2681 - val_my_auc: 0.5821\n",
            " epoch:2 auc: 0.5884\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.7077 - accuracy: 0.6328 - my_auc: 0.6009 - val_loss: 0.6866 - val_accuracy: 0.3509 - val_my_auc: 0.5771\n",
            " epoch:3 auc: 0.5813\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6563 - accuracy: 0.6797 - my_auc: 0.6423 - val_loss: 0.6835 - val_accuracy: 0.3844 - val_my_auc: 0.5935\n",
            " epoch:4 auc: 0.5959\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6441 - accuracy: 0.6667 - my_auc: 0.6340 - val_loss: 0.6795 - val_accuracy: 0.4624 - val_my_auc: 0.6036\n",
            " epoch:5 auc: 0.6023\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 0.6333 - accuracy: 0.6681 - my_auc: 0.6645 - val_loss: 0.6753 - val_accuracy: 0.4961 - val_my_auc: 0.6105\n",
            " epoch:6 auc: 0.6122\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6474 - accuracy: 0.6554 - my_auc: 0.6734 - val_loss: 0.6707 - val_accuracy: 0.5795 - val_my_auc: 0.6142\n",
            " epoch:7 auc: 0.6165\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6086 - accuracy: 0.7205 - my_auc: 0.6877 - val_loss: 0.6672 - val_accuracy: 0.5643 - val_my_auc: 0.6211\n",
            " epoch:8 auc: 0.6205\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6316 - accuracy: 0.6588 - my_auc: 0.7125 - val_loss: 0.6657 - val_accuracy: 0.6362 - val_my_auc: 0.6185\n",
            " epoch:9 auc: 0.6173\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6530 - accuracy: 0.6692 - my_auc: 0.6796 - val_loss: 0.6626 - val_accuracy: 0.6454 - val_my_auc: 0.6227\n",
            " epoch:10 auc: 0.6228\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6411 - accuracy: 0.6593 - my_auc: 0.6938 - val_loss: 0.6628 - val_accuracy: 0.7046 - val_my_auc: 0.6248\n",
            " epoch:11 auc: 0.6255\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6405 - accuracy: 0.6893 - my_auc: 0.7212 - val_loss: 0.6628 - val_accuracy: 0.6728 - val_my_auc: 0.6250\n",
            " epoch:12 auc: 0.6248\n",
            "Epoch 14/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6423 - accuracy: 0.6648 - my_auc: 0.6973 - val_loss: 0.6665 - val_accuracy: 0.7089 - val_my_auc: 0.6231\n",
            " epoch:13 auc: 0.6229\n",
            "Epoch 15/30\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 0.5737 - accuracy: 0.7096 - my_auc: 0.7614 - val_loss: 0.6645 - val_accuracy: 0.6855 - val_my_auc: 0.6287\n",
            " epoch:14 auc: 0.6293\n",
            "Epoch 16/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6074 - accuracy: 0.6755 - my_auc: 0.7162 - val_loss: 0.6624 - val_accuracy: 0.6815 - val_my_auc: 0.6340\n",
            " epoch:15 auc: 0.6342\n",
            "Epoch 17/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.5628 - accuracy: 0.6900 - my_auc: 0.7315 - val_loss: 0.6644 - val_accuracy: 0.6690 - val_my_auc: 0.6324\n",
            " epoch:16 auc: 0.6326\n",
            "Epoch 18/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6128 - accuracy: 0.6656 - my_auc: 0.7546 - val_loss: 0.6713 - val_accuracy: 0.7366 - val_my_auc: 0.6309\n",
            " epoch:17 auc: 0.6313\n",
            "Epoch 19/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6086 - accuracy: 0.7044 - my_auc: 0.7464 - val_loss: 0.6692 - val_accuracy: 0.7175 - val_my_auc: 0.6361\n",
            " epoch:18 auc: 0.6359\n",
            "Epoch 20/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.5868 - accuracy: 0.7092 - my_auc: 0.7385 - val_loss: 0.6666 - val_accuracy: 0.6782 - val_my_auc: 0.6368\n",
            " epoch:19 auc: 0.6374\n",
            "Epoch 21/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6133 - accuracy: 0.6743 - my_auc: 0.7322 - val_loss: 0.6741 - val_accuracy: 0.7217 - val_my_auc: 0.6351\n",
            " epoch:20 auc: 0.6355\n",
            "Epoch 22/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.5919 - accuracy: 0.7046 - my_auc: 0.7683 - val_loss: 0.6770 - val_accuracy: 0.7251 - val_my_auc: 0.6342\n",
            " epoch:21 auc: 0.6348\n",
            "Epoch 23/30\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.6243 - accuracy: 0.6874 - my_auc: 0.7499 - val_loss: 0.6837 - val_accuracy: 0.7467 - val_my_auc: 0.6310\n",
            " epoch:22 auc: 0.6308\n",
            "AUC:  0.6358\n",
            "avg_AUC :  0.6033563611049654\n",
            "avg_AUC_2 :  0.6031257537518457\n",
            "('wrd+spc', ['Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis', 's1', 'pastavg3', 'cr3'])\n",
            "0.6033563611049654\n",
            "['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3', 'Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis']\n",
            "iterate_num:  1 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 57)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 28ms/step - loss: 0.7424 - accuracy: 0.8817 - my_auc: 0.4970 - val_loss: 0.6824 - val_accuracy: 0.1536 - val_my_auc: 0.6391\n",
            " epoch:0 auc: 0.6424\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6779 - accuracy: 0.6649 - my_auc: 0.6334 - val_loss: 0.6731 - val_accuracy: 0.2404 - val_my_auc: 0.6636\n",
            " epoch:1 auc: 0.6631\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6348 - accuracy: 0.7045 - my_auc: 0.7096 - val_loss: 0.6624 - val_accuracy: 0.2536 - val_my_auc: 0.6850\n",
            " epoch:2 auc: 0.6866\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6394 - accuracy: 0.6630 - my_auc: 0.7175 - val_loss: 0.6455 - val_accuracy: 0.3987 - val_my_auc: 0.7020\n",
            " epoch:3 auc: 0.7013\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5886 - accuracy: 0.7269 - my_auc: 0.7665 - val_loss: 0.6356 - val_accuracy: 0.4182 - val_my_auc: 0.7152\n",
            " epoch:4 auc: 0.7152\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5800 - accuracy: 0.7049 - my_auc: 0.7435 - val_loss: 0.6247 - val_accuracy: 0.4722 - val_my_auc: 0.7249\n",
            " epoch:5 auc: 0.7250\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6256 - accuracy: 0.6653 - my_auc: 0.7762 - val_loss: 0.6161 - val_accuracy: 0.5479 - val_my_auc: 0.7288\n",
            " epoch:6 auc: 0.7294\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5618 - accuracy: 0.7267 - my_auc: 0.7930 - val_loss: 0.6148 - val_accuracy: 0.4893 - val_my_auc: 0.7358\n",
            " epoch:7 auc: 0.7357\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5558 - accuracy: 0.6804 - my_auc: 0.7837 - val_loss: 0.6091 - val_accuracy: 0.5571 - val_my_auc: 0.7357\n",
            " epoch:8 auc: 0.7356\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5346 - accuracy: 0.7013 - my_auc: 0.7974 - val_loss: 0.6087 - val_accuracy: 0.5358 - val_my_auc: 0.7403\n",
            " epoch:9 auc: 0.7399\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.5623 - accuracy: 0.6689 - my_auc: 0.8014 - val_loss: 0.6060 - val_accuracy: 0.6166 - val_my_auc: 0.7375\n",
            " epoch:10 auc: 0.7380\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5301 - accuracy: 0.7127 - my_auc: 0.8175 - val_loss: 0.6094 - val_accuracy: 0.6606 - val_my_auc: 0.7377\n",
            " epoch:11 auc: 0.7372\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5289 - accuracy: 0.7252 - my_auc: 0.8185 - val_loss: 0.6222 - val_accuracy: 0.7243 - val_my_auc: 0.7350\n",
            " epoch:12 auc: 0.7349\n",
            "AUC:  0.7013\n",
            "iterate_num:  2 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 57)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 30ms/step - loss: 0.7642 - accuracy: 0.7556 - my_auc: 0.5126 - val_loss: 0.6813 - val_accuracy: 0.6505 - val_my_auc: 0.6720\n",
            " epoch:0 auc: 0.6721\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.6408 - accuracy: 0.7636 - my_auc: 0.6343 - val_loss: 0.6719 - val_accuracy: 0.1941 - val_my_auc: 0.7166\n",
            " epoch:1 auc: 0.7188\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.6407 - accuracy: 0.6881 - my_auc: 0.6906 - val_loss: 0.6592 - val_accuracy: 0.3498 - val_my_auc: 0.7261\n",
            " epoch:2 auc: 0.7267\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5810 - accuracy: 0.7448 - my_auc: 0.7103 - val_loss: 0.6465 - val_accuracy: 0.3958 - val_my_auc: 0.7344\n",
            " epoch:3 auc: 0.7330\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6162 - accuracy: 0.6838 - my_auc: 0.7192 - val_loss: 0.6335 - val_accuracy: 0.4678 - val_my_auc: 0.7439\n",
            " epoch:4 auc: 0.7440\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6072 - accuracy: 0.7027 - my_auc: 0.7438 - val_loss: 0.6214 - val_accuracy: 0.5690 - val_my_auc: 0.7444\n",
            " epoch:5 auc: 0.7445\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5568 - accuracy: 0.7466 - my_auc: 0.7683 - val_loss: 0.6138 - val_accuracy: 0.5533 - val_my_auc: 0.7466\n",
            " epoch:6 auc: 0.7472\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5776 - accuracy: 0.7062 - my_auc: 0.7418 - val_loss: 0.6100 - val_accuracy: 0.5668 - val_my_auc: 0.7446\n",
            " epoch:7 auc: 0.7443\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6086 - accuracy: 0.6740 - my_auc: 0.7661 - val_loss: 0.6109 - val_accuracy: 0.6635 - val_my_auc: 0.7379\n",
            " epoch:8 auc: 0.7378\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.5452 - accuracy: 0.7404 - my_auc: 0.7787 - val_loss: 0.6079 - val_accuracy: 0.5906 - val_my_auc: 0.7388\n",
            " epoch:9 auc: 0.7391\n",
            "AUC:  0.6846\n",
            "iterate_num:  3 \n",
            " sum of test_y: 102.0\n",
            "train_x_shape (27366, 57)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 30ms/step - loss: 0.7278 - accuracy: 0.8343 - my_auc: 0.5613 - val_loss: 0.6857 - val_accuracy: 0.0449 - val_my_auc: 0.6693\n",
            " epoch:0 auc: 0.6701\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6423 - accuracy: 0.7414 - my_auc: 0.6272 - val_loss: 0.6855 - val_accuracy: 0.0612 - val_my_auc: 0.6835\n",
            " epoch:1 auc: 0.6809\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6695 - accuracy: 0.6374 - my_auc: 0.6923 - val_loss: 0.6636 - val_accuracy: 0.2761 - val_my_auc: 0.6949\n",
            " epoch:2 auc: 0.6946\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.6094 - accuracy: 0.7533 - my_auc: 0.7150 - val_loss: 0.6552 - val_accuracy: 0.3145 - val_my_auc: 0.7025\n",
            " epoch:3 auc: 0.7017\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6467 - accuracy: 0.6585 - my_auc: 0.7310 - val_loss: 0.6394 - val_accuracy: 0.4478 - val_my_auc: 0.7117\n",
            " epoch:4 auc: 0.7115\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5982 - accuracy: 0.7146 - my_auc: 0.7530 - val_loss: 0.6283 - val_accuracy: 0.5165 - val_my_auc: 0.7126\n",
            " epoch:5 auc: 0.7120\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.5861 - accuracy: 0.7095 - my_auc: 0.7552 - val_loss: 0.6222 - val_accuracy: 0.5351 - val_my_auc: 0.7190\n",
            " epoch:6 auc: 0.7189\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.5830 - accuracy: 0.6918 - my_auc: 0.7740 - val_loss: 0.6121 - val_accuracy: 0.5816 - val_my_auc: 0.7252\n",
            " epoch:7 auc: 0.7255\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.5482 - accuracy: 0.7060 - my_auc: 0.7925 - val_loss: 0.6080 - val_accuracy: 0.6358 - val_my_auc: 0.7266\n",
            " epoch:8 auc: 0.7268\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5531 - accuracy: 0.7200 - my_auc: 0.7784 - val_loss: 0.6045 - val_accuracy: 0.6451 - val_my_auc: 0.7318\n",
            " epoch:9 auc: 0.7315\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5552 - accuracy: 0.7048 - my_auc: 0.7926 - val_loss: 0.6101 - val_accuracy: 0.6894 - val_my_auc: 0.7245\n",
            " epoch:10 auc: 0.7249\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5395 - accuracy: 0.7183 - my_auc: 0.8030 - val_loss: 0.6119 - val_accuracy: 0.6595 - val_my_auc: 0.7238\n",
            " epoch:11 auc: 0.7238\n",
            "Epoch 13/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5261 - accuracy: 0.7056 - my_auc: 0.8069 - val_loss: 0.6216 - val_accuracy: 0.6741 - val_my_auc: 0.7159\n",
            " epoch:12 auc: 0.7161\n",
            "AUC:  0.7288\n",
            "iterate_num:  4 \n",
            " sum of test_y: 101.0\n",
            "train_x_shape (27367, 57)\n",
            "Epoch 1/30\n",
            "54/54 [==============================] - 3s 30ms/step - loss: 0.7093 - accuracy: 0.5754 - my_auc: 0.5040 - val_loss: 0.6886 - val_accuracy: 0.0262 - val_my_auc: 0.6470\n",
            " epoch:0 auc: 0.6476\n",
            "Epoch 2/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6623 - accuracy: 0.6510 - my_auc: 0.6639 - val_loss: 0.6786 - val_accuracy: 0.1628 - val_my_auc: 0.6758\n",
            " epoch:1 auc: 0.6766\n",
            "Epoch 3/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.6792 - accuracy: 0.6591 - my_auc: 0.6948 - val_loss: 0.6642 - val_accuracy: 0.4467 - val_my_auc: 0.6964\n",
            " epoch:2 auc: 0.6955\n",
            "Epoch 4/30\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.5921 - accuracy: 0.7684 - my_auc: 0.7198 - val_loss: 0.6558 - val_accuracy: 0.3975 - val_my_auc: 0.6986\n",
            " epoch:3 auc: 0.6980\n",
            "Epoch 5/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5992 - accuracy: 0.7220 - my_auc: 0.7225 - val_loss: 0.6452 - val_accuracy: 0.4626 - val_my_auc: 0.7034\n",
            " epoch:4 auc: 0.7037\n",
            "Epoch 6/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5501 - accuracy: 0.7339 - my_auc: 0.7718 - val_loss: 0.6365 - val_accuracy: 0.5105 - val_my_auc: 0.7044\n",
            " epoch:5 auc: 0.7051\n",
            "Epoch 7/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5697 - accuracy: 0.7070 - my_auc: 0.7842 - val_loss: 0.6264 - val_accuracy: 0.5818 - val_my_auc: 0.7069\n",
            " epoch:6 auc: 0.7069\n",
            "Epoch 8/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5777 - accuracy: 0.7065 - my_auc: 0.7706 - val_loss: 0.6201 - val_accuracy: 0.6019 - val_my_auc: 0.7085\n",
            " epoch:7 auc: 0.7086\n",
            "Epoch 9/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.5754 - accuracy: 0.6904 - my_auc: 0.8022 - val_loss: 0.6183 - val_accuracy: 0.6631 - val_my_auc: 0.7086\n",
            " epoch:8 auc: 0.7081\n",
            "Epoch 10/30\n",
            "54/54 [==============================] - 1s 11ms/step - loss: 0.5303 - accuracy: 0.7358 - my_auc: 0.8032 - val_loss: 0.6226 - val_accuracy: 0.6918 - val_my_auc: 0.7041\n",
            " epoch:9 auc: 0.7046\n",
            "Epoch 11/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.5394 - accuracy: 0.7359 - my_auc: 0.8143 - val_loss: 0.6217 - val_accuracy: 0.6646 - val_my_auc: 0.7033\n",
            " epoch:10 auc: 0.7034\n",
            "Epoch 12/30\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.5273 - accuracy: 0.7138 - my_auc: 0.8216 - val_loss: 0.6314 - val_accuracy: 0.7230 - val_my_auc: 0.7025\n",
            " epoch:11 auc: 0.7029\n",
            "AUC:  0.7209\n",
            "avg_AUC :  0.7089143433737806\n",
            "avg_AUC_2 :  0.7089983508154736\n",
            "('perf+spc+wrd', ['aqi', 'asset_turnover', 'depi', 'gmi', 'opm', 'rg', 'sg', 'sgee', 'ch_rec', 'ch_inv', 'soft_asset', 'ch_cs', 'ch_roa', 'issue', 's1', 'pastavg3', 'cr3', 'Litigious_3_p', 'Litigious_3_n', 'Litigious_3_new', 'Litigious_3_dis', 'Uncertainty_3_p', 'Uncertainty_3_n', 'Uncertainty_3_new', 'Uncertainty_3_dis', 'StrongModal_3_p', 'StrongModal_3_n', 'StrongModal_3_new', 'StrongModal_3_dis', 'Reward_3_p', 'Reward_3_n', 'Reward_3_new', 'Reward_3_dis', 'Achieve_3_p', 'Achieve_3_n', 'Achieve_3_new', 'Achieve_3_dis', 'Positive_3_p', 'Positive_3_n', 'Positive_3_new', 'Positive_3_dis', 'Discrep_3_p', 'Discrep_3_n', 'Discrep_3_new', 'Discrep_3_dis', 'Compare_3_p', 'Compare_3_n', 'Compare_3_new', 'Compare_3_dis', 'Negative_3_p', 'Negative_3_n', 'Negative_3_new', 'Negative_3_dis', 'WeakModal_3_p', 'WeakModal_3_n', 'WeakModal_3_new', 'WeakModal_3_dis'])\n",
            "0.7089143433737806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk2YpCVFO6H4"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkzIKyXuCtyq"
      },
      "source": [
        "# LSTM+CNN+TCN\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLnGyW7J_Yg"
      },
      "source": [
        "# def model_cnn(w, num_wrds, num_depth):\r\n",
        "#   model = None\r\n",
        "#   input_wds = Input(shape=(w, num_wrds, 1),dtype='float32', name='input_words')\r\n",
        "\r\n",
        "#   cov = Conv2D(4, (2,2), activation='relu')(input_wds)\r\n",
        "#   maxpool = MaxPooling2D((1,1))(cov)\r\n",
        "#   #reshp = Reshape((1,64))(maxpool)\r\n",
        "#   #flt = Flatten()(maxpool)\r\n",
        "#   model = Model(inputs=input_wds, outputs= maxpool)\r\n",
        "#   model._name = 'cnn'          \r\n",
        "#   model.summary()\r\n",
        "#   return model\r\n",
        "\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVlUupiwsRkI"
      },
      "source": [
        "# def model_lstm(n1, n2, n3, w, num_wrds, num_perf, num_depth):\r\n",
        "#     model = None\r\n",
        "    \r\n",
        "#     input_perf = Input(shape=(1,num_perf),dtype='float32', name='input_financial')\r\n",
        "#     input_wds = Input(shape=(w,num_wrds),dtype='float32', name='input_words')\r\n",
        "\r\n",
        "#     cnnmodel = model_cnn(w, num_wrds, num_depth)\r\n",
        "#     cnn_wrds = cnnmodel(input_wds)\r\n",
        "\r\n",
        "#     reshp_perf = Reshape((1,num_perf))(input_perf)\r\n",
        "\r\n",
        "#     concate = Concatenate(axis=-1)([cnn_wrds,input_perf])\r\n",
        "\r\n",
        "#     #nor = BatchNormalization()(concate)\r\n",
        "#     drop1 = Dropout(0.3)(concate)\r\n",
        "#     #nor = BatchNormalization()(drop1)\r\n",
        "#     # LSTM_w_1 = Bidirectional(LSTM(81,recurrent_dropout = 0.2, name = 'layer_lstm_1',\\\r\n",
        "#     #                   return_sequences=True, \\\r\n",
        "#     #                #kernel_regularizer=regularizers.l2(0.01),\\\r\n",
        "#     #                #bias_regularizer=regularizers.l2(0.01),\r\n",
        "#     #               activity_regularizer=regularizers.l2(0.01)))(drop1)\r\n",
        "#     LSTM_w_2 = LSTM(81,recurrent_dropout = 0.2, name = 'layer_lstm_2', \\\r\n",
        "#                 return_sequences=False )(drop1)        \r\n",
        "#     #nor = BatchNormalization()(LSTM_w_2)\r\n",
        "#     dense1 = Dense(81, activation='relu', name='dense1')(LSTM_w_2)\r\n",
        "#     dense2 = Dense(n3, activation='relu', name='dense2')(dense1)\r\n",
        "#     nor = BatchNormalization()(dense2)\r\n",
        "#     drop2 = Dropout(0.3)(nor)\r\n",
        "    \r\n",
        "#     preds = Dense(1, activation='sigmoid', name='output')(drop2)\r\n",
        "#     model = Model(inputs=[input_perf, input_wds], outputs=preds)\r\n",
        "#     model._name = \"lstm\"\r\n",
        "#     model.summary()\r\n",
        "    \r\n",
        "#     return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8RP1jytmqrO"
      },
      "source": [
        "#LR\r\n",
        "# def model_cnn_lstm(n1, n2, n3, T, perf, words, channel, filters = 24):\r\n",
        "#     model = None\r\n",
        "    \r\n",
        "#     input_perf = Input(shape=(T,perf), \\\r\n",
        "#                       dtype='float32', name='input_perf')\r\n",
        "    \r\n",
        "#     input_words = Input(shape=(T, words, channel), \\\r\n",
        "#                       dtype='float32', name='input_words') \r\n",
        "    \r\n",
        "#     word_norm = BatchNormalization()(input_words)\r\n",
        "    \r\n",
        "    \r\n",
        "#     conv = Conv2D(filters = filters,\\\r\n",
        "#                   kernel_size = 1,\\\r\n",
        "#                   use_bias = False,\\\r\n",
        "#                   activation = None)(word_norm)  # Shape None x T x words x filters\r\n",
        "#     print(\"conv shape:\", K.int_shape(conv))\r\n",
        "#     conv = BatchNormalization()(conv)\r\n",
        "#     conv = Activation(\"relu\") (conv)\r\n",
        "    \r\n",
        "#     pool = MaxPooling2D(pool_size=(1, words), \\\r\n",
        "#                         strides=(1,1))(conv) # None x T x filters\r\n",
        "#     pool = Reshape((T, filters))(conv)\r\n",
        "#     print(\"pool shape:\", K.int_shape(pool))\r\n",
        "    \r\n",
        "#     #drop = Dropout(0.3)(input_all)\r\n",
        "#     # Now concatenate\r\n",
        "#     perf_norm = BatchNormalization()(input_perf)\r\n",
        "#     all_input = Concatenate(axis = -1)([perf_norm, pool])\r\n",
        "#     #nor = BatchNormalization()(all_input)\r\n",
        "    \r\n",
        "#     LSTM_w_1 = Bidirectional(LSTM(n1, dropout= 0.3, recurrent_dropout = 0.3,\\\r\n",
        "#                     name = 'layer_lstm_1', return_sequences=True))(all_input)\r\n",
        "#     LSTM_w_2 = LSTM(n2, dropout= 0.3, recurrent_dropout = 0.3,\r\n",
        "#                    name = 'layer_lstm_2', return_sequences=False)(LSTM_w_1)\r\n",
        "    \r\n",
        "#     dense = Dense(n3, activation='relu', name='dense')(LSTM_w_2)\r\n",
        "#     drop = Dropout(0.3)(dense)\r\n",
        "    \r\n",
        "#     preds = Dense(1, activation='sigmoid', name='output')(drop)\r\n",
        "#     model = Model(inputs=[input_perf, input_words], outputs=preds)\r\n",
        "#     model._name = \"model_cnnlstm\"\r\n",
        "#     model.summary()\r\n",
        "    \r\n",
        "#     return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDstFvvF2Jav"
      },
      "source": [
        "def model_cnn_lstm_1(n1,n2,n3,T, perf, words, types ,filters = 32): #channel,\r\n",
        "    model = None\r\n",
        "    filters = 32\r\n",
        "    \r\n",
        "    input_perf = Input(shape=(T,perf), \\\r\n",
        "                      dtype='float32', name='input_perf')\r\n",
        "    \r\n",
        "    input_words = Input(shape=(T,words,types), \\\r\n",
        "                      dtype='float32', name='input_words')\r\n",
        "    \r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(input_words)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 1:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    pool_1 = MaxPooling2D(pool_size= (1,17), \\\r\n",
        "                        strides=1)(conv)\r\n",
        "    pool_1 = Reshape((T, filters))(pool_1)\r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 2:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    pool_2 = MaxPooling2D(pool_size= (1,14), \\\r\n",
        "                        strides=1)(conv)\r\n",
        "    pool_2 = Reshape((T, filters))(pool_2)\r\n",
        "\r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 3:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    pool_3 = MaxPooling2D(pool_size= (1,11), \\\r\n",
        "                        strides=1)(conv)\r\n",
        "    pool_3 = Reshape((T, filters))(pool_3)\r\n",
        "    \r\n",
        "    # conv = Conv2D(filters = filters,\\\r\n",
        "    #               kernel_size = (1,4),\\\r\n",
        "    #               activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    # print(\"conv shape 4:\", K.int_shape(conv))\r\n",
        "    # conv = BatchNormalization()(conv)\r\n",
        "    # conv = Activation(\"relu\") (conv)\r\n",
        "    # pool_4 = MaxPooling2D(pool_size= (1,8), \\\r\n",
        "    #                     strides=1)(conv)\r\n",
        "    # pool_4 = Reshape((T, filters))(pool_4)\r\n",
        "    \r\n",
        "    # pool = MaxPooling2D(pool_size= (1,8), \\\r\n",
        "    #                     strides=1)(conv) # None x T x filters\r\n",
        "    \r\n",
        "    # print(\"pool_init shape:\", K.int_shape(pool))\r\n",
        "    # pool = Reshape((T, filters))(pool)\r\n",
        "    # print(\"pool shape:\", K.int_shape(pool))\r\n",
        "    \r\n",
        "    #drop = Dropout(0.3)(input_all)\r\n",
        "    # Now concatenate\r\n",
        "    perf_norm = BatchNormalization()(input_perf)\r\n",
        "    all_input = Concatenate(axis = -1)([perf_norm, pool_3, pool_2, pool_1 ])\r\n",
        "    nor = BatchNormalization()(all_input)\r\n",
        "    #drop = Dropout(0.3)(nor)\r\n",
        "    \r\n",
        "    LSTM_w_1 =Bidirectional(LSTM(120,  recurrent_dropout = 0.2,\r\n",
        "                    name = 'layer_lstm_1', return_sequences=True,\r\n",
        "                    kernel_regularizer = regularizers.l2(0.01),\r\n",
        "                    activity_regularizer = regularizers.l2(0.01),\r\n",
        "                    bias_regularizer = regularizers.l2(0.01)\r\n",
        "                    ))(nor)\r\n",
        "    LSTM_w_2 = LSTM(120, recurrent_dropout = 0.2, \r\n",
        "                   name = 'layer_lstm_2', return_sequences=False)(LSTM_w_1)\r\n",
        "\r\n",
        "    dense = Dense(32, activation='relu', name='dense')(LSTM_w_2)\r\n",
        "    nor = BatchNormalization()(dense)\r\n",
        "    drop = Dropout(0.3)(nor)\r\n",
        "    preds = Dense(1, activation='sigmoid', name='output')(drop)\r\n",
        "    model = Model(inputs=[input_perf, input_words], outputs=preds)\r\n",
        "    model._name = \"model_cnnlstm_1\"\r\n",
        "    model.summary()\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4bg7qkX98EY"
      },
      "source": [
        "def model_cnn_lstm_2(n1,n2,n3,T, perf, words, types ,filters = 32): #channel,\r\n",
        "    model = None\r\n",
        "    filters = 32\r\n",
        "    \r\n",
        "    input_perf = Input(shape=(T,perf), \\\r\n",
        "                      dtype='float32', name='input_perf')\r\n",
        "    \r\n",
        "    input_words = Input(shape=(T,words,types), \\\r\n",
        "                      dtype='float32', name='input_words')\r\n",
        "    \r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(input_words)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 1:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 2:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "\r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 3:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    \r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 4:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    \r\n",
        "    pool = MaxPooling2D(pool_size= (1,8), \\\r\n",
        "                        strides=1)(conv) # None x T x filters\r\n",
        "    \r\n",
        "    print(\"pool_init shape:\", K.int_shape(pool))\r\n",
        "    pool = Reshape((T, filters))(pool)\r\n",
        "    print(\"pool shape:\", K.int_shape(pool))\r\n",
        "    \r\n",
        "    #drop = Dropout(0.3)(input_all)\r\n",
        "    # Now concatenate\r\n",
        "    perf_norm = BatchNormalization()(input_perf)\r\n",
        "    all_input = Concatenate(axis = -1)([perf_norm, pool ])\r\n",
        "    nor = BatchNormalization()(all_input)\r\n",
        "    #drop = Dropout(0.3)(nor)\r\n",
        "    \r\n",
        "    LSTM_w_1 =Bidirectional(LSTM(n1,  recurrent_dropout = 0.2,\r\n",
        "                    name = 'layer_lstm_1', return_sequences=True,\r\n",
        "                    kernel_regularizer = regularizers.l2(0.01),\r\n",
        "                    activity_regularizer = regularizers.l2(0.01),\r\n",
        "                    bias_regularizer = regularizers.l2(0.01)\r\n",
        "                    ))(nor)\r\n",
        "    LSTM_w_2 = LSTM(n2, recurrent_dropout = 0.2, \r\n",
        "                   name = 'layer_lstm_2', return_sequences=False)(LSTM_w_1)\r\n",
        "\r\n",
        "    dense = Dense(32, activation='relu', name='dense')(LSTM_w_2)\r\n",
        "    nor = BatchNormalization()(dense)\r\n",
        "    drop = Dropout(0.3)(nor)\r\n",
        "    preds = Dense(1, activation='sigmoid', name='output')(drop)\r\n",
        "    model = Model(inputs=[input_perf, input_words], outputs=preds)\r\n",
        "    model._name = \"model_cnnlstm_2\"\r\n",
        "    model.summary()\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G_3E_OX3uc0"
      },
      "source": [
        "def model_cnn_lstm_3(n1,n2,n3,T, perf, words, types ,filters = 32): #channel,\r\n",
        "    model = None\r\n",
        "    filters = 32\r\n",
        "    \r\n",
        "    input_perf = Input(shape=(T,perf), \\\r\n",
        "                      dtype='float32', name='input_perf')\r\n",
        "    \r\n",
        "    input_words = Input(shape=(T,words,types), \\\r\n",
        "                      dtype='float32', name='input_words')\r\n",
        "    \r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(input_words)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 1:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    pool = MaxPooling2D(pool_size= (1,4), \\\r\n",
        "                        strides=1)(conv)\r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(pool)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 2:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    pool = MaxPooling2D(pool_size= (1,4), \\\r\n",
        "                        strides=1)(conv)\r\n",
        "\r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,4),\\\r\n",
        "                  activation = 'relu')(pool)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape 3:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    \r\n",
        "    # conv = Conv2D(filters = filters,\\\r\n",
        "    #               kernel_size = (1,4),\\\r\n",
        "    #               activation = 'relu')(conv)  # Shape None x T x words x filters\r\n",
        "    # print(\"conv shape 4:\", K.int_shape(conv))\r\n",
        "    # conv = BatchNormalization()(conv)\r\n",
        "    # conv = Activation(\"relu\") (conv)\r\n",
        "    \r\n",
        "    pool = MaxPooling2D(pool_size= (1,5), \\\r\n",
        "                        strides=1)(conv) # None x T x filters\r\n",
        "    \r\n",
        "    print(\"pool_init shape:\", K.int_shape(pool))\r\n",
        "    pool = Reshape((T, filters))(pool)\r\n",
        "    print(\"pool shape:\", K.int_shape(pool))\r\n",
        "    \r\n",
        "    #drop = Dropout(0.3)(input_all)\r\n",
        "    # Now concatenate\r\n",
        "    perf_norm = BatchNormalization()(input_perf)\r\n",
        "    all_input = Concatenate(axis = -1)([perf_norm, pool ])\r\n",
        "    nor = BatchNormalization()(all_input)\r\n",
        "    #drop = Dropout(0.3)(nor)\r\n",
        "    \r\n",
        "    LSTM_w_1 =Bidirectional(LSTM(n1,  recurrent_dropout = 0.2,\r\n",
        "                    name = 'layer_lstm_1', return_sequences=True,\r\n",
        "                    kernel_regularizer = regularizers.l2(0.01),\r\n",
        "                    activity_regularizer = regularizers.l2(0.01),\r\n",
        "                    bias_regularizer = regularizers.l2(0.01)\r\n",
        "                    ))(nor)\r\n",
        "    LSTM_w_2 = LSTM(n2, recurrent_dropout = 0.2, \r\n",
        "                   name = 'layer_lstm_2', return_sequences=False)(LSTM_w_1)\r\n",
        "\r\n",
        "    dense = Dense(32, activation='relu', name='dense')(LSTM_w_2)\r\n",
        "    nor = BatchNormalization()(dense)\r\n",
        "    drop = Dropout(0.3)(nor)\r\n",
        "    preds = Dense(1, activation='sigmoid', name='output')(drop)\r\n",
        "    model = Model(inputs=[input_perf, input_words], outputs=preds)\r\n",
        "    model._name = \"model_cnnlstm_3\"\r\n",
        "    model.summary()\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWiCYABO24qC"
      },
      "source": [
        "def model_tcn(n1,n2,n3,T, perf, words, channel, filters = 24, dropout = 0):\r\n",
        "    model = None\r\n",
        "    \r\n",
        "    input_perf = Input(shape=(T,perf), \\\r\n",
        "                      dtype='float32', name='input_perf')\r\n",
        "    \r\n",
        "    input_words = Input(shape=(T,words,channel), \\\r\n",
        "                      dtype='float32', name='input_words') \r\n",
        "    \r\n",
        "    word_norm = BatchNormalization()(input_words)\r\n",
        "    padding = ZeroPadding2D(padding=((0,0),(words-1,0)))(word_norm)\r\n",
        "    \r\n",
        "    print(\"shape after padding:\", K.int_shape(padding))\r\n",
        "    \r\n",
        "    conv = Conv2D(filters = filters,\\\r\n",
        "                  kernel_size = (1,words),\\\r\n",
        "                  use_bias = False,\\\r\n",
        "                  activation = None)(padding)  # Shape None x T x words x filters\r\n",
        "    print(\"conv shape:\", K.int_shape(conv))\r\n",
        "    conv = BatchNormalization()(conv)\r\n",
        "    conv = Activation(\"relu\") (conv)\r\n",
        "    \r\n",
        "    pool = MaxPooling2D(pool_size=(1, words), \\\r\n",
        "                        strides=(1,1))(conv) # None x T x filters\r\n",
        "    pool = Reshape((T, filters))(pool)\r\n",
        "    print(\"pool shape:\", K.int_shape(pool))\r\n",
        "\r\n",
        "    #drop = Dropout(0.3)(input_all)\r\n",
        "    # Now concatenate\r\n",
        "    perf_norm = BatchNormalization()(input_perf)\r\n",
        "    all_input = Concatenate(axis = -1)([perf_norm, pool])\r\n",
        "    #all_rsp = Reshape((filters+15,))(all_input)\r\n",
        "    #nor = BatchNormalization()(all_input)\r\n",
        "    \r\n",
        "    LSTM_w_1 =Bidirectional(LSTM(n1,  recurrent_dropout = 0.2,\r\n",
        "                    name = 'layer_lstm_1', return_sequences=True,\r\n",
        "                    # kernel_regularizer = regularizers.l2(0.01),\r\n",
        "                    # activity_regularizer = regularizers.l2(0.01),\r\n",
        "                    # bias_regularizer = regularizers.l2(0.01)\r\n",
        "                    ))(all_input)\r\n",
        "    LSTM_w_2 = LSTM(n2, dropout= 0.3, recurrent_dropout = 0.3,\r\n",
        "                   name = 'layer_lstm_2', return_sequences=False)(LSTM_w_1)\r\n",
        "    \r\n",
        "    dense = Dense(n3, activation='relu', name='dense')(LSTM_w_2)\r\n",
        "    drop = Dropout(0.3)(dense)\r\n",
        "    \r\n",
        "    preds = Dense(1, activation='sigmoid', name='output')(drop)\r\n",
        "    model = Model(inputs=[input_perf, input_words], outputs=preds)\r\n",
        "    model._name = \"model_tcn\"\r\n",
        "    model.summary()\r\n",
        "    \r\n",
        "    return model\r\n",
        "    "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "744Oq8vYvbsF"
      },
      "source": [
        "def performance_measure(pred_yp, y):\r\n",
        "    '''\r\n",
        "    Given lists of predicted y probability and x, y, return a dataframe of AR, AUC, Brier, Decile Table\r\n",
        "    '''\r\n",
        "    \r\n",
        "    tenc_dat = pd.DataFrame({'y_true':y,'probability':pred_yp.flatten()})\r\n",
        "    tenc_dat.sort_values('probability',axis = 0,ascending=False, inplace = True)\r\n",
        "    tenc_dat.index = range(0,len(tenc_dat))\r\n",
        "    y = tenc_dat['y_true']\r\n",
        "    point = float(len(tenc_dat))/10\r\n",
        "    point = int(round(point))\r\n",
        "    tenc = []\r\n",
        "    for i in range(0,10):\r\n",
        "        tenc.append(y[(i*point):((i+1)*point)])\r\n",
        "    tenc[9]=tenc[9].append(y[10*point:])\r\n",
        "    total = sum(y)\r\n",
        "    num_of_bkr = []\r\n",
        "    for j in range(0,10):\r\n",
        "        num_of_bkr.append(sum(tenc[j]))\r\n",
        "    tencile_bkr = np.array(num_of_bkr)\r\n",
        "    rate = tencile_bkr.astype(float)/total\r\n",
        "\r\n",
        "    return rate"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1oy1Jcrvd1Y"
      },
      "source": [
        "class AUCEvaluation(Callback):\r\n",
        "    \"\"\" Show AUC after interval number of epoches \"\"\"\r\n",
        "    def __init__(self, validation_data=(), interval=1):\r\n",
        "        super(Callback, self).__init__()\r\n",
        "        self.interval = interval\r\n",
        "        self.X_val, self.y_val = validation_data\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if epoch % self.interval == 0:\r\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\r\n",
        "            score = roc_auc_score(self.y_val, y_pred)\r\n",
        "            logs['auc'] = score\r\n",
        "            tencile=performance_measure(y_pred, self.y_val)\r\n",
        "            logs['tencile'] = tencile\r\n",
        "            print(\" epoch:{:d} auc: {:.4f}\".format(epoch, score))\r\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gmZEw24vv9R"
      },
      "source": [
        "def fit_model( model,x_train, y_train, x_val, y_val, x_test, y_test, model_name, verbose = 0):\r\n",
        "    \r\n",
        "    history = None\r\n",
        "    result = None\r\n",
        "    \r\n",
        "    model_df = model\r\n",
        "    print('in fit, val_y: ', sum(y_val), 'test_y',sum(y_test), 'train_y',sum(y_train))\r\n",
        "\r\n",
        "    #class weight for test \r\n",
        "    class_weights = class_weight.compute_class_weight('balanced',\r\n",
        "                              np.unique(y_train),\r\n",
        "                              y_train)\r\n",
        "    class_weights = {l:c for l,c in zip(np.unique(y_train), class_weights)}\r\n",
        "    # class weight for val\r\n",
        "    val_class_weights = class_weight.compute_class_weight('balanced',\r\n",
        "                                np.unique(y_val),\r\n",
        "                                y_val)\r\n",
        "    val_class_weights = {l:c for l,c in zip(np.unique(y_val), val_class_weights)}\r\n",
        "    val_sample_weights = []\r\n",
        "    for y in y_val:\r\n",
        "        if y == 1:\r\n",
        "          val_sample_weights.append(val_class_weights[1])\r\n",
        "        else: val_sample_weights.append(val_class_weights[0])\r\n",
        "    val_sample_weights = np.asarray(val_sample_weights)\r\n",
        "    #print('in fit val weights', val_sample_weights.shape)\r\n",
        "    \r\n",
        "    auc_eval = AUCEvaluation(validation_data=(x_val, y_val), interval=1)\r\n",
        "    earlyStopping = EarlyStopping(monitor='val_my_auc',patience = 3, \r\n",
        "                      verbose =verbose, mode ='max')\r\n",
        "    checkpoint = ModelCheckpoint(model_name,monitor='val_my_auc',verbose=verbose,\r\n",
        "              save_best_only=True, save_weights_only=True, mode ='max')\r\n",
        "\r\n",
        "    opt = optimizers.RMSprop(lr=0.0005)\r\n",
        "    model_df.compile(optimizer= opt,\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy', metrics.AUC(name='my_auc')])\r\n",
        "\r\n",
        "    history = model_df.fit(x_train,\r\n",
        "                y_train,\r\n",
        "                epochs=15,\r\n",
        "                batch_size=128,\r\n",
        "                verbose =verbose,\r\n",
        "                #callbacks=[auc_eval],\\\r\n",
        "                callbacks=[auc_eval, earlyStopping, checkpoint], #checkpoint\r\n",
        "                class_weight = class_weights, \\\r\n",
        "                validation_data=(x_val, y_val, val_sample_weights)) \r\n",
        "    \r\n",
        "    model_df.load_weights(model_name)\r\n",
        "    y_pred = model_df.predict(x_test)\r\n",
        "    y_pred_1=np.reshape(y_pred, -1)\r\n",
        "    auc_score = roc_auc_score(y_test, y_pred_1)\r\n",
        "    #auc_score = roc_auc_score(y_test, y_pred_1)\r\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_1)\r\n",
        "    print(\"AUC: \" ,round(auc_score,4))\r\n",
        "    print([ round(h,4) for h in history.history['val_loss']])    \r\n",
        "    plt.plot(history.history['loss'])\r\n",
        "    plt.plot(history.history['val_loss'])\r\n",
        "    plt.title('loss')\r\n",
        "    plt.ylabel('loss')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.legend(['train', 'val'], loc='upper left')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "        \r\n",
        "    return history, auc_score, fpr, tpr, thresholds, [y_test, y_pred_1]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo3ShzQyoSci"
      },
      "source": [
        "def cross_val(data, label, perf_cols, words_cols, name, w, filters = 24):\r\n",
        "    tprs = []\r\n",
        "    auc_list = []\r\n",
        "    mean_fpr = np.linspace(0,1,100)\r\n",
        "    predicted_res =[]\r\n",
        "    \r\n",
        "    kf = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 42)\r\n",
        "    c = 0\r\n",
        "\r\n",
        "    X_perf, X_word, Y = shift_data(df_fl, w, \\\r\n",
        "                    perf_cols, \\\r\n",
        "                    words_cols, \\\r\n",
        "                    'label')\r\n",
        "    \r\n",
        "    for train_index, test_index in kf.split(range(len(X_perf)),Y):\r\n",
        "        \r\n",
        "        c += 1\r\n",
        "        \r\n",
        "        l1 = X_perf.shape[-1]+ filters + 16\r\n",
        "        \r\n",
        "        model = model_tcn(l1,l1,32,\\\r\n",
        "                    X_perf.shape[1],\\\r\n",
        "                    X_perf.shape[2],\r\n",
        "                    X_word.shape[2], \\\r\n",
        "                    X_word.shape[3], \\\r\n",
        "                    filters = 24)\r\n",
        "    \r\n",
        "        #model = model_lstm(n1,n2,n,w)\r\n",
        "        train_x, train_val = train_test_split(train_index,test_size=0.2, \\\r\n",
        "                              random_state=42, stratify = Y[train_index])\r\n",
        "        train_perf_data = X_perf[train_x]\r\n",
        "        train_word_data = X_word[train_x]\r\n",
        "        train_perf_val = X_perf[train_val]\r\n",
        "        train_word_val = X_word[train_val]\r\n",
        "        train_label = Y[train_x]\r\n",
        "        val_label = Y[train_val]\r\n",
        "\r\n",
        "        test_perf_x = X_perf[test_index]\r\n",
        "        test_word_x = X_word[test_index]\r\n",
        "        test_y = Y[test_index] \r\n",
        "        \r\n",
        "        train_data = [train_perf_data, train_word_data]\r\n",
        "        val_data = [train_perf_val,train_word_val]\r\n",
        "        test_x = [test_perf_x, test_word_x]\r\n",
        "\r\n",
        "        mod_res = fit_model(model, train_data, train_label, val_data, val_label, test_x, test_y,\\\r\n",
        "                  name+'_'+str(c)+'_lstm_model')\r\n",
        "        history = mod_res[0]\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "        auc_list.append(mod_res[1])\r\n",
        "        tprs.append(np.interp(mean_fpr, mod_res[2], mod_res[3]))\r\n",
        "        temp_pred_res = [mod_res[-1],test_index]\r\n",
        "        print(len(test_index))\r\n",
        "        predicted_res.append(temp_pred_res)\r\n",
        "        \r\n",
        "        \r\n",
        "    mean_tpr = np.mean(tprs, axis=0)\r\n",
        "    print('avg_AUC : ', np.mean(auc_list))\r\n",
        "    print('avg_AUC_2 : ', auc(mean_fpr, mean_tpr))\r\n",
        "    \r\n",
        "    return np.average(auc_list),mean_tpr,predicted_res"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rfF_t6WWVcu"
      },
      "source": [
        "#a = np.arange(16).reshape((2,8#))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4FwTZ8JWZcJ"
      },
      "source": [
        "#a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooJMEa21WZem"
      },
      "source": [
        "#a.reshape((2,8), order = '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ub73u2WZhz"
      },
      "source": [
        "#a.reshape((2,4,2), order = 'C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSPkGEAWZjk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDLCeoltzOn6"
      },
      "source": [
        "def shift_data(data, step, perf_cols, words_cols, label):\r\n",
        "    A = []\r\n",
        "    \r\n",
        "    cols = perf_cols + words_cols\r\n",
        "    A.append(data[cols].values)\r\n",
        "    \r\n",
        "    for t in range(1, step):\r\n",
        "        d = data.groupby(\"cik\")[cols].shift(t)\r\n",
        "        A.append(d.values)\r\n",
        "    A = A[::-1]\r\n",
        "    A = np.concatenate(A, axis = 1)  # flatten shifted columns\r\n",
        "    A = np.concatenate([data[label].values[:,None], A], axis = 1)  # add target\r\n",
        "    #np.random.shuffle(A)\r\n",
        "    #print(A[3])\r\n",
        "    print('init A shape: ',A.shape)\r\n",
        "    #print(A[3,:])\r\n",
        "    #print(data[cols].iloc[4,:])\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    A = A[~np.isnan(A).any(axis=1)]  # drop nan\r\n",
        "    \r\n",
        "\r\n",
        "    Y = A[:,0]  # get target\r\n",
        "    #A = np.reshape(A[:,1:], (len(A), step, len(cols))) # reshape\r\n",
        "\r\n",
        "    A = np.reshape(A[:,1:], (len(A), step, len(perf_cols) + len(words_cols)))\r\n",
        "    \r\n",
        "    A_perf = A[:, :, 0:len(perf_cols)]\r\n",
        "    # CNN_LSTM must be None x T x words x 4\r\n",
        "    A_words = A[:, :, len(perf_cols):]\r\n",
        "    #convert the shape to [[_p,_new][_dis,_n]]\r\n",
        "    A_words = A_words.reshape((len(A), step, int(len(words_cols)/2), 2), order = 'C')\r\n",
        "    \r\n",
        "    print(A_perf.shape, A_words.shape, Y.sum())\r\n",
        "    return A_perf, A_words, Y\r\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDR1cndI_xJF"
      },
      "source": [
        "#shift_perf,shit_wrd, _y = shift_data(df_fl, 3, v_perf + v_1+ v_2, selected_new_all_sorted, label='label')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B42iqO2dZWQU"
      },
      "source": [
        "#shit_wrd[2]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJJvwrMuANlK"
      },
      "source": [
        "# for k , fr in df_fl.groupby(\"cik\"):\r\n",
        "#   print(fr[selected_new_all_sorted])\r\n",
        "#   break"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaVhYNFbZJgE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fREeiK36ZJi7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xCGS_HCd2CiB",
        "cellView": "code",
        "outputId": "47f8e0e2-abc6-46f7-bc5c-44bb7de16049"
      },
      "source": [
        "\n",
        "#@title Default title text\n",
        "print(\"\\n ======= Select columns for LSTM ALL OBS.=========\")\n",
        "\n",
        "result = []\n",
        "\n",
        "# First choose all varaibles\n",
        "#cols = v_perf + v_1+ v_2 + selected_new_all #must order like this\n",
        "w = 3\n",
        "perf_cols = v_perf + v_1\n",
        "words_cols = selected_new_all_sorted\n",
        "label = 'label'\n",
        "h = cross_val( df_fl, 'label', \\\n",
        "          perf_cols,\\\n",
        "          words_cols, \\\n",
        "          'cnn_lstm',w, \\\n",
        "          filters = 64)\n",
        "    \n",
        "best_auc = h[0]\n",
        "\n",
        "# Next, remove one from the list each time\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ======= Select columns for LSTM ALL OBS.=========\n",
            "init A shape:  (53635, 166)\n",
            "(39123, 3, 15) (39123, 3, 20, 2) 341.0\n",
            "shape after padding: (None, 3, 39, 2)\n",
            "conv shape: (None, 3, 20, 24)\n",
            "pool shape: (None, 3, 24)\n",
            "Model: \"model_tcn\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_words (InputLayer)        [(None, 3, 20, 2)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 3, 20, 2)     8           input_words[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 3, 39, 2)     0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 3, 20, 24)    960         zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 3, 20, 24)    96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 3, 20, 24)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "input_perf (InputLayer)         [(None, 3, 15)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 3, 1, 24)     0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 3, 15)        60          input_perf[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 3, 24)        0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 39)        0           batch_normalization_27[0][0]     \n",
            "                                                                 reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 3, 190)       102600      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "layer_lstm_2 (LSTM)             (None, 95)           108680      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           3072        layer_lstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            33          dropout_25[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 215,509\n",
            "Trainable params: 215,427\n",
            "Non-trainable params: 82\n",
            "__________________________________________________________________________________________________\n",
            "in fit, val_y:  51.0 test_y 85.0 train_y 205.0\n",
            " epoch:0 auc: 0.6905\n",
            " epoch:1 auc: 0.7206\n",
            " epoch:2 auc: 0.7499\n",
            " epoch:3 auc: 0.7731\n",
            " epoch:4 auc: 0.7655\n",
            " epoch:5 auc: 0.7477\n",
            " epoch:6 auc: 0.7829\n",
            " epoch:7 auc: 0.7689\n",
            " epoch:8 auc: 0.7185\n",
            " epoch:9 auc: 0.7521\n",
            "AUC:  0.7771\n",
            "[0.6768, 0.629, 0.5888, 0.5774, 0.5766, 0.59, 0.581, 0.855, 0.7181, 1.2223]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9781\n",
            "shape after padding: (None, 3, 39, 2)\n",
            "conv shape: (None, 3, 20, 24)\n",
            "pool shape: (None, 3, 24)\n",
            "Model: \"model_tcn\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_words (InputLayer)        [(None, 3, 20, 2)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 3, 20, 2)     8           input_words[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 3, 39, 2)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 3, 20, 24)    960         zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 3, 20, 24)    96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 3, 20, 24)    0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "input_perf (InputLayer)         [(None, 3, 15)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 3, 1, 24)     0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 3, 15)        60          input_perf[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 3, 24)        0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 39)        0           batch_normalization_30[0][0]     \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 3, 190)       102600      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_lstm_2 (LSTM)             (None, 95)           108680      bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           3072        layer_lstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            33          dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 215,509\n",
            "Trainable params: 215,427\n",
            "Non-trainable params: 82\n",
            "__________________________________________________________________________________________________\n",
            "in fit, val_y:  51.0 test_y 85.0 train_y 205.0\n",
            " epoch:0 auc: 0.7252\n",
            " epoch:1 auc: 0.7294\n",
            " epoch:2 auc: 0.7501\n",
            " epoch:3 auc: 0.7645\n",
            " epoch:4 auc: 0.7907\n",
            " epoch:5 auc: 0.7911\n",
            " epoch:6 auc: 0.8040\n",
            " epoch:7 auc: 0.8017\n",
            " epoch:8 auc: 0.8139\n",
            " epoch:9 auc: 0.8091\n",
            " epoch:10 auc: 0.7972\n",
            " epoch:11 auc: 0.8116\n",
            "AUC:  0.7907\n",
            "[0.6764, 0.643, 0.6185, 0.6136, 0.5843, 0.6043, 0.6101, 0.6203, 0.6179, 0.6831, 0.8236, 0.832]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9781\n",
            "shape after padding: (None, 3, 39, 2)\n",
            "conv shape: (None, 3, 20, 24)\n",
            "pool shape: (None, 3, 24)\n",
            "Model: \"model_tcn\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_words (InputLayer)        [(None, 3, 20, 2)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 3, 20, 2)     8           input_words[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 3, 39, 2)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 3, 20, 24)    960         zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 3, 20, 24)    96          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 3, 20, 24)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "input_perf (InputLayer)         [(None, 3, 15)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 3, 1, 24)     0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 3, 15)        60          input_perf[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 3, 24)        0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3, 39)        0           batch_normalization_33[0][0]     \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 3, 190)       102600      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_lstm_2 (LSTM)             (None, 95)           108680      bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           3072        layer_lstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            33          dropout_27[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 215,509\n",
            "Trainable params: 215,427\n",
            "Non-trainable params: 82\n",
            "__________________________________________________________________________________________________\n",
            "in fit, val_y:  51.0 test_y 86.0 train_y 204.0\n",
            " epoch:0 auc: 0.7350\n",
            " epoch:1 auc: 0.7603\n",
            " epoch:2 auc: 0.7450\n",
            " epoch:3 auc: 0.7759\n",
            " epoch:4 auc: 0.7954\n",
            " epoch:5 auc: 0.8094\n",
            " epoch:6 auc: 0.8125\n",
            " epoch:7 auc: 0.7947\n",
            " epoch:8 auc: 0.7929\n",
            " epoch:9 auc: 0.7903\n",
            "AUC:  0.7793\n",
            "[0.6778, 0.6161, 0.7735, 0.5786, 0.5515, 0.6131, 0.5644, 0.801, 0.6676, 1.3823]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9781\n",
            "shape after padding: (None, 3, 39, 2)\n",
            "conv shape: (None, 3, 20, 24)\n",
            "pool shape: (None, 3, 24)\n",
            "Model: \"model_tcn\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_words (InputLayer)        [(None, 3, 20, 2)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 3, 20, 2)     8           input_words[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 3, 39, 2)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 3, 20, 24)    960         zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 3, 20, 24)    96          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 3, 20, 24)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "input_perf (InputLayer)         [(None, 3, 15)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 3, 1, 24)     0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 3, 15)        60          input_perf[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 3, 24)        0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3, 39)        0           batch_normalization_36[0][0]     \n",
            "                                                                 reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 3, 190)       102600      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_lstm_2 (LSTM)             (None, 95)           108680      bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           3072        layer_lstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            33          dropout_28[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 215,509\n",
            "Trainable params: 215,427\n",
            "Non-trainable params: 82\n",
            "__________________________________________________________________________________________________\n",
            "in fit, val_y:  51.0 test_y 85.0 train_y 205.0\n",
            " epoch:0 auc: 0.6731\n",
            " epoch:1 auc: 0.7005\n",
            " epoch:2 auc: 0.7118\n",
            " epoch:3 auc: 0.7336\n",
            " epoch:4 auc: 0.7373\n",
            " epoch:5 auc: 0.7474\n",
            " epoch:6 auc: 0.7635\n",
            " epoch:7 auc: 0.7573\n",
            " epoch:8 auc: 0.7452\n",
            " epoch:9 auc: 0.7537\n",
            "AUC:  0.7618\n",
            "[0.6744, 0.6288, 0.6695, 0.6232, 0.5972, 0.5964, 0.6071, 0.9634, 0.6832, 0.6504]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9780\n",
            "avg_AUC :  0.7772358719987336\n",
            "avg_AUC_2 :  0.7772226367643604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GazZJdEKg20L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cec7bcd-7629-4038-9138-fdd41f0de6a3"
      },
      "source": [
        "#best_auc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.752735165519567"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSUyWOb4BXMq"
      },
      "source": [
        "\r\n",
        "#all_cols = selected_new_all\r\n",
        "\r\n",
        "# selected_cols = selected_new_all + v_1 + v_2\r\n",
        "# random.shuffle(selected_cols)\r\n",
        "\r\n",
        "# for col in selected_cols:\r\n",
        "    \r\n",
        "#     print(\"\\n ===========Test {}============\\n\".format(col))\r\n",
        "    \r\n",
        "#     test_cols = [item for item in selected_cols if item !=col]\r\n",
        "#     #test_cols = v_perf + v_1+ v_2 + [col]\r\n",
        "    \r\n",
        "#     # n1 = 2*len(test_cols)\r\n",
        "#     # n2 = len(test_cols)\r\n",
        "#     label = 'label'\r\n",
        "#     h = cross_val( df_fl, label, test_cols + v_perf, 'lstm_wds', 2)\r\n",
        "\r\n",
        "#     gap = h[0] - best_auc\r\n",
        "#     print('****** gap: {0:.4f}'.format(gap))\r\n",
        "    \r\n",
        "#     result.append([col, h[0], best_auc, gap]) #gap\r\n",
        "    \r\n",
        "#     if h[0]>= best_auc:  # better to drop col\r\n",
        "        \r\n",
        "#         selected_cols = test_cols\r\n",
        "        \r\n",
        "#         best_auc = h[0]\r\n",
        "        \r\n",
        "#         print(\"*** {} removed ***\".format(col))\r\n",
        "    \r\n",
        "#     print(\"current auc: {0:.4f}\\tbest_auc: {1:.4f}\\tcolumn: {2}\".format(h[0], best_auc, col))\r\n",
        "#     print(\"selected columns: \", selected_cols)\r\n",
        "\r\n",
        "\r\n",
        "# pd.DataFrame(result, columns = [\"column\", \"auc\", \"prev_auc\",\"gap\"]).to_csv(\"var_selection_lstm_bi_2.csv\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DSUDXmCJjAM"
      },
      "source": [
        "print(random.shuffle(selected_new_all))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZtYwaE4J-zF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c73139-ac7c-4e72-850f-22424b79fc8d"
      },
      "source": [
        "'Litigious_3_dis' in list(df_fl.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tygu3EpmnZl7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}